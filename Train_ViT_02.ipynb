{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/visionCodes/blob/master/Train_ViT_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# google/vit-base-patch16-224 on fused train dataset using Adam lr=1e-5"
      ],
      "metadata": {
        "id": "quQ4Wf-5rN4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!gdown https://drive.google.com/uc?id=1wgl3QGXZ4m2aLg3T-1TDXQqSP31RuXgL"
      ],
      "metadata": {
        "id": "9F-pNaIFdRHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip \"/content/hateful_train+test_unseen.zip\""
      ],
      "metadata": {
        "id": "nvujlUCYdi0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6yV2GxQWZw4L"
      },
      "outputs": [],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import transforms\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "KKSy44hXZx8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your training and validation data\n",
        "train_data_root = \"/content/hateful_ViT1/train\"\n",
        "val_data_root = \"/content/hateful_ViT1/test\""
      ],
      "metadata": {
        "id": "LCC0cmnIZzTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTImageProcessor\n",
        "\n",
        "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]\n",
        "\n",
        "# Define transformations for the input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_mean, std=image_std)\n",
        "])\n"
      ],
      "metadata": {
        "id": "VcVrx7RkZzdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset using ImageFolder and apply transformations\n",
        "train_dataset = ImageFolder(train_data_root, transform=transform)\n",
        "val_dataset = ImageFolder(val_data_root, transform=transform)"
      ],
      "metadata": {
        "id": "Ysx46DEHZznC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label2id and id2label dictionaries based on the class names in the dataset\n",
        "label2id = {class_name: idx for class_name, idx in train_dataset.class_to_idx.items()}\n",
        "id2label = {idx: class_name for class_name, idx in train_dataset.class_to_idx.items()}"
      ],
      "metadata": {
        "id": "x0bNIiTMZzwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define batch size and number of workers (adjust based on your system's resources)\n",
        "batch_size = 32"
      ],
      "metadata": {
        "id": "w9LlN21zyr07"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader for the dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "l9zA4YBKZ0SA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(train_dataset.classes)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "id": "3y3_g9PHZ0by"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", id2label=id2label, label2id=label2id, num_labels=num_classes, ignore_mismatched_sizes=True)\n",
        "vit.classifier = nn.Linear(vit.config.hidden_size, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit.to(device)\n",
        "print(vit)"
      ],
      "metadata": {
        "id": "d5Yqjhf2Z0kl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(vit.parameters(), lr=1e-5)\n",
        "# optimizer = optim.SGD(vit.parameters(), lr=0.001, momentum=0.9)\n",
        "num_epochs = 20"
      ],
      "metadata": {
        "id": "JdqAwRBSaEGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes = []\n",
        "actual_labels = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    vit.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        inputs = processor(images=images, return_tensors=\"pt\")\n",
        "        outputs = model(**inputs)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Calculate average loss for this epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    save_dir = \"/content/Train-ViT-02/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    model_name = str(epoch+1) + \"_model.pth\"\n",
        "    save_path = os.path.join(save_dir, model_name)  # Specify the complete path to the model file\n",
        "    torch.save(vit.state_dict(), save_path)\n",
        "\n",
        "    # Validation\n",
        "    vit.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            inputs = processor(images=images, return_tensors=\"pt\")\n",
        "            outputs = model(**inputs)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            total_val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.logits, 1)\n",
        "            correct_predictions += (predicted == labels).sum().item()\n",
        "\n",
        "            predicted_classes.extend(predicted.cpu().numpy())\n",
        "            actual_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average loss and accuracy for validation set\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Compute evaluation metrics using the predicted_classes and actual_labels lists\n",
        "    accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "    precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "    recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "    f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "    print(classification_report(actual_labels, predicted_classes))\n",
        "    cm = confusion_matrix(actual_labels, predicted_classes)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    auroc = roc_auc_score(actual_labels, predicted_classes)\n",
        "    print(\"AUROC:\", auroc)\n",
        "\n"
      ],
      "metadata": {
        "id": "NVhg3NjSaGwr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4dPB8YABeK7K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "4GQ9rAC39wZ7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import transforms\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from PIL import Image\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "metadata": {
        "id": "FkrmYqLG9xNO",
        "outputId": "3e9fcae2-22be-45af-bbc2-bb677bc763d2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your training and validation data\n",
        "train_data_root = \"/content/hateful_ViT1/train\"\n",
        "val_data_root = \"/content/hateful_ViT1/test\""
      ],
      "metadata": {
        "id": "iuzq1QSi9xb7"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_folder = ImageFolder(root_dir, transform=None)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_folder)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.image_folder.imgs[idx]\n",
        "        image = self.load_function(image_path)\n",
        "        return image, label\n",
        "\n",
        "    def load_function(self, path):\n",
        "        # Load the image in RGB format using PIL\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        # Resize the image to the required input size for the ViT model\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        image = transform(image)\n",
        "        return image\n",
        "\n",
        "def custom_collate_fn(batch):\n",
        "    images, labels = zip(*batch)\n",
        "    images = torch.stack(images, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "IHaz_D389xe0"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = CustomImageDataset(train_data_root)\n",
        "val_dataset = CustomImageDataset(val_data_root)"
      ],
      "metadata": {
        "id": "wWKnCSRxAPMT"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 32"
      ],
      "metadata": {
        "id": "qZqy_EtVAdrv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create DataLoader for the dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=custom_collate_fn)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,  collate_fn=custom_collate_fn)"
      ],
      "metadata": {
        "id": "BYo3qrDy9xhI"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Merge the class_to_idx dictionaries from both train and validation datasets\n",
        "label2id = {class_name: idx for class_name, idx in train_dataset.image_folder.class_to_idx.items()}\n",
        "id2label = {idx: class_name for class_name, idx in label2id.items()}\n"
      ],
      "metadata": {
        "id": "t2UDuOsa9xjz"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(train_dataset.image_folder.classes)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "id": "c2S3gGe6_C8S",
        "outputId": "1cc9371d-30aa-40cd-c571-e761e794ef34",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTImageProcessor\n",
        "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n",
        "vit = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", id2label=id2label, label2id=label2id, num_labels=num_classes, ignore_mismatched_sizes=True)\n",
        "vit.classifier = nn.Linear(vit.config.hidden_size, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit.to(device)\n",
        "print(vit)"
      ],
      "metadata": {
        "id": "D-VIQKcB_DhN",
        "outputId": "0c73eede-b1b4-4b55-97f7-a185be684c0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([2, 768]) in the model instantiated\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([2]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViTForImageClassification(\n",
            "  (vit): ViTModel(\n",
            "    (embeddings): ViTEmbeddings(\n",
            "      (patch_embeddings): ViTPatchEmbeddings(\n",
            "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      )\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): ViTEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x ViTLayer(\n",
            "          (attention): ViTAttention(\n",
            "            (attention): ViTSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): ViTSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): ViTIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): ViTOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = AdamW(vit.parameters(), lr=1e-5)\n",
        "# optimizer = optim.SGD(vit.parameters(), lr=0.001, momentum=0.9)\n",
        "num_epochs = 20"
      ],
      "metadata": {
        "id": "HRe2XIkV_Dt9",
        "outputId": "98670bd9-7aac-4ba4-c7b9-57c6c4859070",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes = []\n",
        "actual_labels = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    vit.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        inputs = processor(images=images, return_tensors=\"pt\")\n",
        "        inputs = inputs.to(device)\n",
        "        outputs = vit(**inputs)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "        # total_loss += loss.item()\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Calculate average loss for this epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    save_dir = \"/content/Train-ViT-02/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    model_name = str(epoch+1) + \"_model.pth\"\n",
        "    save_path = os.path.join(save_dir, model_name)  # Specify the complete path to the model file\n",
        "    torch.save(vit.state_dict(), save_path)\n",
        "\n",
        "    # Validation\n",
        "    vit.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            inputs = processor(images=images, return_tensors=\"pt\")\n",
        "            inputs = inputs.to(device)\n",
        "            outputs = vit(**inputs)\n",
        "            loss = criterion(outputs.logits, labels)\n",
        "            val_loss += loss.item()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.logits, 1)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            predicted_classes.extend(predicted.cpu().numpy())\n",
        "            actual_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average loss and accuracy for validation set\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Compute evaluation metrics using the predicted_classes and actual_labels lists\n",
        "    accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "    precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "    recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "    f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "    print(classification_report(actual_labels, predicted_classes))\n",
        "    cm = confusion_matrix(actual_labels, predicted_classes)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    auroc = roc_auc_score(actual_labels, predicted_classes)\n",
        "    print(\"AUROC:\", auroc)\n"
      ],
      "metadata": {
        "id": "tKZVbsqZ_OxS",
        "outputId": "fa7b144f-0b5c-476b-dc68-31afa5c70dd8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Training Loss: 0.6593 - Validation Loss: 0.0209 - Accuracy: 0.6250\n",
            "Accuracy: 0.625\n",
            "Precision: 0.390625\n",
            "Recall: 0.625\n",
            "F1-score: 0.4807692307692308\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      1.00      0.77      1250\n",
            "           1       0.00      0.00      0.00       750\n",
            "\n",
            "    accuracy                           0.62      2000\n",
            "   macro avg       0.31      0.50      0.38      2000\n",
            "weighted avg       0.39      0.62      0.48      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1250    0]\n",
            " [ 750    0]]\n",
            "AUROC: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20 - Training Loss: 0.6564 - Validation Loss: 0.0211 - Accuracy: 0.6250\n",
            "Accuracy: 0.625\n",
            "Precision: 0.390625\n",
            "Recall: 0.625\n",
            "F1-score: 0.4807692307692308\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      1.00      0.77      2500\n",
            "           1       0.00      0.00      0.00      1500\n",
            "\n",
            "    accuracy                           0.62      4000\n",
            "   macro avg       0.31      0.50      0.38      4000\n",
            "weighted avg       0.39      0.62      0.48      4000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2500    0]\n",
            " [1500    0]]\n",
            "AUROC: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20 - Training Loss: 0.6543 - Validation Loss: 0.0208 - Accuracy: 0.6250\n",
            "Accuracy: 0.625\n",
            "Precision: 0.390625\n",
            "Recall: 0.625\n",
            "F1-score: 0.4807692307692308\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.62      1.00      0.77      3750\n",
            "           1       0.00      0.00      0.00      2250\n",
            "\n",
            "    accuracy                           0.62      6000\n",
            "   macro avg       0.31      0.50      0.38      6000\n",
            "weighted avg       0.39      0.62      0.48      6000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3750    0]\n",
            " [2250    0]]\n",
            "AUROC: 0.5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UN1J9MwcBUjk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}