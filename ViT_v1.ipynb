{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "MC9WyekJuDVY",
        "7qhkw9YUuEWB",
        "XCpn-Kd8uEw8",
        "csBoSDktturt",
        "33qLgBmktvD4",
        "lUzb7QX2tvfq"
      ],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN8GmSbr/UHfx/RJbsbv6g+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/visionCodes/blob/master/ViT_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kEHGeEWitWoK",
        "outputId": "6dc37c19-20ef-4207-90d6-5870582edbfa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wgl3QGXZ4m2aLg3T-1TDXQqSP31RuXgL\n",
            "To: /content/hateful_ViT1.zip\n",
            "100% 3.27G/3.27G [00:49<00:00, 66.6MB/s]\n"
          ]
        }
      ],
      "source": [
        "!gdown https://drive.google.com/uc?id=1wgl3QGXZ4m2aLg3T-1TDXQqSP31RuXgL"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/hateful_ViT1.zip"
      ],
      "metadata": {
        "id": "Nv1ll7Ey2sUH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# data_utils.py"
      ],
      "metadata": {
        "id": "MC9WyekJuDVY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "\n",
        "import torch\n",
        "\n",
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, RandomSampler, DistributedSampler, SequentialSampler\n",
        "logger = logging.getLogger(__name__)\n",
        "def get_loader(args):\n",
        "    if args.local_rank not in [-1, 0]:\n",
        "        torch.distributed.barrier()\n",
        "\n",
        "    transform_train = transforms.Compose([\n",
        "        transforms.RandomResizedCrop((args.img_size, args.img_size), scale=(0.05, 1.0)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "    transform_test = transforms.Compose([\n",
        "        transforms.Resize((args.img_size, args.img_size)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]),\n",
        "    ])\n",
        "    dataset_path = args.dataset_path\n",
        "\n",
        "    train_dir = dataset_path+\"/train/\"\n",
        "    test_dir = dataset_path+\"/test/\"\n",
        "    train_data = datasets.ImageFolder(train_dir, transform=transform_train)\n",
        "    test_data = datasets.ImageFolder(test_dir, transform=transform_test)\n",
        "    if args.local_rank == 0:\n",
        "        torch.distributed.barrier()\n",
        "    train_loader = DataLoader(train_data,\n",
        "                              batch_size=args.train_batch_size,\n",
        "                              shuffle=True,\n",
        "                              num_workers=2,\n",
        "                              pin_memory=True)\n",
        "    test_loader = DataLoader(test_data,\n",
        "                             batch_size=args.eval_batch_size,\n",
        "                             num_workers=2,\n",
        "                             shuffle=False,\n",
        "                             pin_memory=True)\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "j_kbTaInuFOT"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# dist_util.py"
      ],
      "metadata": {
        "id": "7qhkw9YUuEWB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.distributed as dist\n",
        "\n",
        "def get_rank():\n",
        "    if not dist.is_available():\n",
        "        return 0\n",
        "    if not dist.is_initialized():\n",
        "        return 0\n",
        "    return dist.get_rank()\n",
        "\n",
        "def get_world_size():\n",
        "    if not dist.is_available():\n",
        "        return 1\n",
        "    if not dist.is_initialized():\n",
        "        return 1\n",
        "    return dist.get_world_size()\n",
        "\n",
        "def is_main_process():\n",
        "    return get_rank() == 0\n",
        "\n",
        "def format_step(step):\n",
        "    if isinstance(step, str):\n",
        "        return step\n",
        "    s = \"\"\n",
        "    if len(step) > 0:\n",
        "        s += \"Training Epoch: {} \".format(step[0])\n",
        "    if len(step) > 1:\n",
        "        s += \"Training Iteration: {} \".format(step[1])\n",
        "    if len(step) > 2:\n",
        "        s += \"Validation Iteration: {} \".format(step[2])\n",
        "    return s"
      ],
      "metadata": {
        "id": "8L3cJFH-uF3-"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# scheduler.py"
      ],
      "metadata": {
        "id": "XCpn-Kd8uEw8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import math\n",
        "\n",
        "from torch.optim.lr_scheduler import LambdaLR\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "class ConstantLRSchedule(LambdaLR):\n",
        "    \"\"\" Constant learning rate schedule.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, last_epoch=-1):\n",
        "        super(ConstantLRSchedule, self).__init__(optimizer, lambda _: 1.0, last_epoch=last_epoch)\n",
        "\n",
        "\n",
        "class WarmupConstantSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then constant.\n",
        "        Linearly increases learning rate schedule from 0 to 1 over `warmup_steps` training steps.\n",
        "        Keeps learning rate schedule equal to 1. after warmup_steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        super(WarmupConstantSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1.0, self.warmup_steps))\n",
        "        return 1.\n",
        "\n",
        "\n",
        "class WarmupLinearSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then linear decay.\n",
        "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
        "        Linearly decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, t_total, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.t_total = t_total\n",
        "        super(WarmupLinearSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1, self.warmup_steps))\n",
        "        return max(0.0, float(self.t_total - step) / float(max(1.0, self.t_total - self.warmup_steps)))\n",
        "\n",
        "\n",
        "class WarmupCosineSchedule(LambdaLR):\n",
        "    \"\"\" Linear warmup and then cosine decay.\n",
        "        Linearly increases learning rate from 0 to 1 over `warmup_steps` training steps.\n",
        "        Decreases learning rate from 1. to 0. over remaining `t_total - warmup_steps` steps following a cosine curve.\n",
        "        If `cycles` (default=0.5) is different from default, learning rate follows cosine function after warmup.\n",
        "    \"\"\"\n",
        "    def __init__(self, optimizer, warmup_steps, t_total, cycles=.5, last_epoch=-1):\n",
        "        self.warmup_steps = warmup_steps\n",
        "        self.t_total = t_total\n",
        "        self.cycles = cycles\n",
        "        super(WarmupCosineSchedule, self).__init__(optimizer, self.lr_lambda, last_epoch=last_epoch)\n",
        "\n",
        "    def lr_lambda(self, step):\n",
        "        if step < self.warmup_steps:\n",
        "            return float(step) / float(max(1.0, self.warmup_steps))\n",
        "        # progress after warmup\n",
        "        progress = float(step - self.warmup_steps) / float(max(1, self.t_total - self.warmup_steps))\n",
        "        return max(0.0, 0.5 * (1. + math.cos(math.pi * float(self.cycles) * 2.0 * progress)))"
      ],
      "metadata": {
        "id": "3YY3PPjWuGTI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# config.py"
      ],
      "metadata": {
        "id": "csBoSDktturt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ml_collections"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP4iPnYZ3Nzm",
        "outputId": "b2b420f6-5d7f-4637-d8e4-b2a25452bfdc"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml_collections\n",
            "  Downloading ml_collections-0.1.1.tar.gz (77 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/77.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.4.0)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.10/dist-packages (from ml_collections) (6.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from ml_collections) (1.16.0)\n",
            "Requirement already satisfied: contextlib2 in /usr/local/lib/python3.10/dist-packages (from ml_collections) (21.6.0)\n",
            "Building wheels for collected packages: ml_collections\n",
            "  Building wheel for ml_collections (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ml_collections: filename=ml_collections-0.1.1-py3-none-any.whl size=94506 sha256=248ad6b84c78d33a2830c161fe802e33da03546dcbcdab3718af2b6bf7ad5344\n",
            "  Stored in directory: /root/.cache/pip/wheels/7b/89/c9/a9b87790789e94aadcfc393c283e3ecd5ab916aed0a31be8fe\n",
            "Successfully built ml_collections\n",
            "Installing collected packages: ml_collections\n",
            "Successfully installed ml_collections-0.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import ml_collections\n",
        "\n",
        "\n",
        "def get_testing():\n",
        "    \"\"\"Returns a minimal configuration for testing.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
        "    config.hidden_size = 1\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 1\n",
        "    config.transformer.num_heads = 1\n",
        "    config.transformer.num_layers = 1\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_b16_config():\n",
        "    \"\"\"Returns the ViT-B/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
        "    config.hidden_size = 768\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 3072\n",
        "    config.transformer.num_heads = 12\n",
        "    config.transformer.num_layers = 12\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_r50_b16_config():\n",
        "    \"\"\"Returns the Resnet50 + ViT-B/16 configuration.\"\"\"\n",
        "    config = get_b16_config()\n",
        "    del config.patches.size\n",
        "    config.patches.grid = (14, 14)\n",
        "    config.resnet = ml_collections.ConfigDict()\n",
        "    config.resnet.num_layers = (3, 4, 9)\n",
        "    config.resnet.width_factor = 1\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_b32_config():\n",
        "    \"\"\"Returns the ViT-B/32 configuration.\"\"\"\n",
        "    config = get_b16_config()\n",
        "    config.patches.size = (32, 32)\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_l16_config():\n",
        "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (16, 16)})\n",
        "    config.hidden_size = 1024\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 4096\n",
        "    config.transformer.num_heads = 16\n",
        "    config.transformer.num_layers = 24\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_l32_config():\n",
        "    \"\"\"Returns the ViT-L/32 configuration.\"\"\"\n",
        "    config = get_l16_config()\n",
        "    config.patches.size = (32, 32)\n",
        "    return config\n",
        "\n",
        "\n",
        "def get_h14_config():\n",
        "    \"\"\"Returns the ViT-L/16 configuration.\"\"\"\n",
        "    config = ml_collections.ConfigDict()\n",
        "    config.patches = ml_collections.ConfigDict({'size': (14, 14)})\n",
        "    config.hidden_size = 1280\n",
        "    config.transformer = ml_collections.ConfigDict()\n",
        "    config.transformer.mlp_dim = 5120\n",
        "    config.transformer.num_heads = 16\n",
        "    config.transformer.num_layers = 32\n",
        "    config.transformer.attention_dropout_rate = 0.0\n",
        "    config.transformer.dropout_rate = 0.1\n",
        "    config.classifier = 'token'\n",
        "    config.representation_size = None\n",
        "    return config"
      ],
      "metadata": {
        "id": "ekTzQZvbtwib"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# modeling.py"
      ],
      "metadata": {
        "id": "33qLgBmktvD4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import copy\n",
        "import logging\n",
        "import math\n",
        "\n",
        "from os.path import join as pjoin\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "\n",
        "from torch.nn import CrossEntropyLoss, Dropout, Softmax, Linear, Conv2d, LayerNorm\n",
        "from torch.nn.modules.utils import _pair\n",
        "from scipy import ndimage\n",
        "\n",
        "# import models.configs as configs\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "ATTENTION_Q = \"MultiHeadDotProductAttention_1/query\"\n",
        "ATTENTION_K = \"MultiHeadDotProductAttention_1/key\"\n",
        "ATTENTION_V = \"MultiHeadDotProductAttention_1/value\"\n",
        "ATTENTION_OUT = \"MultiHeadDotProductAttention_1/out\"\n",
        "FC_0 = \"MlpBlock_3/Dense_0\"\n",
        "FC_1 = \"MlpBlock_3/Dense_1\"\n",
        "ATTENTION_NORM = \"LayerNorm_0\"\n",
        "MLP_NORM = \"LayerNorm_2\"\n",
        "\n",
        "\n",
        "def np2th(weights, conv=False):\n",
        "    \"\"\"Possibly convert HWIO to OIHW.\"\"\"\n",
        "    if conv:\n",
        "        weights = weights.transpose([3, 2, 0, 1])\n",
        "    return torch.from_numpy(weights)\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": torch.nn.functional.gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}\n",
        "\n",
        "\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Attention, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.num_attention_heads = config.transformer[\"num_heads\"]\n",
        "        self.attention_head_size = int(config.hidden_size / self.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.out = Linear(config.hidden_size, config.hidden_size)\n",
        "        self.attn_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "        self.proj_dropout = Dropout(config.transformer[\"attention_dropout_rate\"])\n",
        "\n",
        "        self.softmax = Softmax(dim=-1)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        attention_probs = self.softmax(attention_scores)\n",
        "        weights = attention_probs if self.vis else None\n",
        "        attention_probs = self.attn_dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        attention_output = self.out(context_layer)\n",
        "        attention_output = self.proj_dropout(attention_output)\n",
        "        return attention_output, weights\n",
        "\n",
        "\n",
        "class Mlp(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(Mlp, self).__init__()\n",
        "        self.fc1 = Linear(config.hidden_size, config.transformer[\"mlp_dim\"])\n",
        "        self.fc2 = Linear(config.transformer[\"mlp_dim\"], config.hidden_size)\n",
        "        self.act_fn = ACT2FN[\"gelu\"]\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        nn.init.xavier_uniform_(self.fc1.weight)\n",
        "        nn.init.xavier_uniform_(self.fc2.weight)\n",
        "        nn.init.normal_(self.fc1.bias, std=1e-6)\n",
        "        nn.init.normal_(self.fc2.bias, std=1e-6)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act_fn(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class Embeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from patch, position embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, img_size, in_channels=3):\n",
        "        super(Embeddings, self).__init__()\n",
        "        self.hybrid = None\n",
        "        img_size = _pair(img_size)\n",
        "\n",
        "        if config.patches.get(\"grid\") is not None:\n",
        "            grid_size = config.patches[\"grid\"]\n",
        "            patch_size = (img_size[0] // 16 // grid_size[0], img_size[1] // 16 // grid_size[1])\n",
        "            n_patches = (img_size[0] // 16) * (img_size[1] // 16)\n",
        "            self.hybrid = True\n",
        "        else:\n",
        "            patch_size = _pair(config.patches[\"size\"])\n",
        "            n_patches = (img_size[0] // patch_size[0]) * (img_size[1] // patch_size[1])\n",
        "            self.hybrid = False\n",
        "\n",
        "        if self.hybrid:\n",
        "            self.hybrid_model = ResNetV2(block_units=config.resnet.num_layers,\n",
        "                                         width_factor=config.resnet.width_factor)\n",
        "            in_channels = self.hybrid_model.width * 16\n",
        "        self.patch_embeddings = Conv2d(in_channels=in_channels,\n",
        "                                       out_channels=config.hidden_size,\n",
        "                                       kernel_size=patch_size,\n",
        "                                       stride=patch_size)\n",
        "        self.position_embeddings = nn.Parameter(torch.zeros(1, n_patches+1, config.hidden_size))\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, config.hidden_size))\n",
        "\n",
        "        self.dropout = Dropout(config.transformer[\"dropout_rate\"])\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "\n",
        "        if self.hybrid:\n",
        "            x = self.hybrid_model(x)\n",
        "        x = self.patch_embeddings(x)\n",
        "        x = x.flatten(2)\n",
        "        x = x.transpose(-1, -2)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "\n",
        "        embeddings = x + self.position_embeddings\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings\n",
        "\n",
        "\n",
        "class Block(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Block, self).__init__()\n",
        "        self.hidden_size = config.hidden_size\n",
        "        self.attention_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        self.ffn = Mlp(config)\n",
        "        self.attn = Attention(config, vis)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h = x\n",
        "        x = self.attention_norm(x)\n",
        "        x, weights = self.attn(x)\n",
        "        x = x + h\n",
        "\n",
        "        h = x\n",
        "        x = self.ffn_norm(x)\n",
        "        x = self.ffn(x)\n",
        "        x = x + h\n",
        "        return x, weights\n",
        "\n",
        "    def load_from(self, weights, n_block):\n",
        "        ROOT = f\"Transformer/encoderblock_{n_block}\"\n",
        "        with torch.no_grad():\n",
        "            query_weight = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            key_weight = np2th(weights[pjoin(ROOT, ATTENTION_K, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            value_weight = np2th(weights[pjoin(ROOT, ATTENTION_V, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "            out_weight = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"kernel\")]).view(self.hidden_size, self.hidden_size).t()\n",
        "\n",
        "            query_bias = np2th(weights[pjoin(ROOT, ATTENTION_Q, \"bias\")]).view(-1)\n",
        "            key_bias = np2th(weights[pjoin(ROOT, ATTENTION_K, \"bias\")]).view(-1)\n",
        "            value_bias = np2th(weights[pjoin(ROOT, ATTENTION_V, \"bias\")]).view(-1)\n",
        "            out_bias = np2th(weights[pjoin(ROOT, ATTENTION_OUT, \"bias\")]).view(-1)\n",
        "\n",
        "            self.attn.query.weight.copy_(query_weight)\n",
        "            self.attn.key.weight.copy_(key_weight)\n",
        "            self.attn.value.weight.copy_(value_weight)\n",
        "            self.attn.out.weight.copy_(out_weight)\n",
        "            self.attn.query.bias.copy_(query_bias)\n",
        "            self.attn.key.bias.copy_(key_bias)\n",
        "            self.attn.value.bias.copy_(value_bias)\n",
        "            self.attn.out.bias.copy_(out_bias)\n",
        "\n",
        "            mlp_weight_0 = np2th(weights[pjoin(ROOT, FC_0, \"kernel\")]).t()\n",
        "            mlp_weight_1 = np2th(weights[pjoin(ROOT, FC_1, \"kernel\")]).t()\n",
        "            mlp_bias_0 = np2th(weights[pjoin(ROOT, FC_0, \"bias\")]).t()\n",
        "            mlp_bias_1 = np2th(weights[pjoin(ROOT, FC_1, \"bias\")]).t()\n",
        "\n",
        "            self.ffn.fc1.weight.copy_(mlp_weight_0)\n",
        "            self.ffn.fc2.weight.copy_(mlp_weight_1)\n",
        "            self.ffn.fc1.bias.copy_(mlp_bias_0)\n",
        "            self.ffn.fc2.bias.copy_(mlp_bias_1)\n",
        "\n",
        "            self.attention_norm.weight.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"scale\")]))\n",
        "            self.attention_norm.bias.copy_(np2th(weights[pjoin(ROOT, ATTENTION_NORM, \"bias\")]))\n",
        "            self.ffn_norm.weight.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"scale\")]))\n",
        "            self.ffn_norm.bias.copy_(np2th(weights[pjoin(ROOT, MLP_NORM, \"bias\")]))\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config, vis):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.vis = vis\n",
        "        self.layer = nn.ModuleList()\n",
        "        self.encoder_norm = LayerNorm(config.hidden_size, eps=1e-6)\n",
        "        for _ in range(config.transformer[\"num_layers\"]):\n",
        "            layer = Block(config, vis)\n",
        "            self.layer.append(copy.deepcopy(layer))\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        attn_weights = []\n",
        "        for layer_block in self.layer:\n",
        "            hidden_states, weights = layer_block(hidden_states)\n",
        "            if self.vis:\n",
        "                attn_weights.append(weights)\n",
        "        encoded = self.encoder_norm(hidden_states)\n",
        "        return encoded, attn_weights\n",
        "\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, config, img_size, vis):\n",
        "        super(Transformer, self).__init__()\n",
        "        self.embeddings = Embeddings(config, img_size=img_size)\n",
        "        self.encoder = Encoder(config, vis)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        embedding_output = self.embeddings(input_ids)\n",
        "        encoded, attn_weights = self.encoder(embedding_output)\n",
        "        return encoded, attn_weights\n",
        "\n",
        "\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, config, img_size=224, num_classes=21843, zero_head=False, vis=False):\n",
        "        super(VisionTransformer, self).__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.zero_head = zero_head\n",
        "        self.classifier = config.classifier\n",
        "        self.configg = config\n",
        "        self.transformer = Transformer(config, img_size, vis)\n",
        "        self.head = Linear(config.hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x, labels=None):\n",
        "        #print(x.size())\n",
        "        x, attn_weights = self.transformer(x)\n",
        "        logits = self.head(x[:, 0])\n",
        "        #print(self.configg)\n",
        "        if labels is not None:\n",
        "            loss_fct = CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_classes), labels.view(-1))\n",
        "            return loss\n",
        "        else:\n",
        "            return logits, attn_weights\n",
        "\n",
        "    def load_from(self, weights):\n",
        "        with torch.no_grad():\n",
        "            if self.zero_head:\n",
        "                nn.init.zeros_(self.head.weight)\n",
        "                nn.init.zeros_(self.head.bias)\n",
        "            else:\n",
        "                self.head.weight.copy_(np2th(weights[\"head/kernel\"]).t())\n",
        "                self.head.bias.copy_(np2th(weights[\"head/bias\"]).t())\n",
        "\n",
        "            self.transformer.embeddings.patch_embeddings.weight.copy_(np2th(weights[\"embedding/kernel\"], conv=True))\n",
        "            self.transformer.embeddings.patch_embeddings.bias.copy_(np2th(weights[\"embedding/bias\"]))\n",
        "            self.transformer.embeddings.cls_token.copy_(np2th(weights[\"cls\"]))\n",
        "            self.transformer.encoder.encoder_norm.weight.copy_(np2th(weights[\"Transformer/encoder_norm/scale\"]))\n",
        "            self.transformer.encoder.encoder_norm.bias.copy_(np2th(weights[\"Transformer/encoder_norm/bias\"]))\n",
        "\n",
        "            posemb = np2th(weights[\"Transformer/posembed_input/pos_embedding\"])\n",
        "            posemb_new = self.transformer.embeddings.position_embeddings\n",
        "            if posemb.size() == posemb_new.size():\n",
        "                self.transformer.embeddings.position_embeddings.copy_(posemb)\n",
        "            else:\n",
        "                logger.info(\"load_pretrained: resized variant: %s to %s\" % (posemb.size(), posemb_new.size()))\n",
        "                ntok_new = posemb_new.size(1)\n",
        "\n",
        "                if self.classifier == \"token\":\n",
        "                    posemb_tok, posemb_grid = posemb[:, :1], posemb[0, 1:]\n",
        "                    ntok_new -= 1\n",
        "                else:\n",
        "                    posemb_tok, posemb_grid = posemb[:, :0], posemb[0]\n",
        "\n",
        "                gs_old = int(np.sqrt(len(posemb_grid)))\n",
        "                gs_new = int(np.sqrt(ntok_new))\n",
        "                print('load_pretrained: grid-size from %s to %s' % (gs_old, gs_new))\n",
        "                posemb_grid = posemb_grid.reshape(gs_old, gs_old, -1)\n",
        "\n",
        "                zoom = (gs_new / gs_old, gs_new / gs_old, 1)\n",
        "                posemb_grid = ndimage.zoom(posemb_grid, zoom, order=1)\n",
        "                posemb_grid = posemb_grid.reshape(1, gs_new * gs_new, -1)\n",
        "                posemb = np.concatenate([posemb_tok, posemb_grid], axis=1)\n",
        "                self.transformer.embeddings.position_embeddings.copy_(np2th(posemb))\n",
        "\n",
        "            for bname, block in self.transformer.encoder.named_children():\n",
        "                for uname, unit in block.named_children():\n",
        "                    unit.load_from(weights, n_block=uname)\n",
        "\n",
        "            if self.transformer.embeddings.hybrid:\n",
        "                self.transformer.embeddings.hybrid_model.root.conv.weight.copy_(np2th(weights[\"conv_root/kernel\"], conv=True))\n",
        "                gn_weight = np2th(weights[\"gn_root/scale\"]).view(-1)\n",
        "                gn_bias = np2th(weights[\"gn_root/bias\"]).view(-1)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.weight.copy_(gn_weight)\n",
        "                self.transformer.embeddings.hybrid_model.root.gn.bias.copy_(gn_bias)\n",
        "\n",
        "                for bname, block in self.transformer.embeddings.hybrid_model.body.named_children():\n",
        "                    for uname, unit in block.named_children():\n",
        "                        unit.load_from(weights, n_block=bname, n_unit=uname)\n",
        "\n",
        "\n",
        "CONFIGS = {\n",
        "    'ViT-B_16': get_b16_config(),\n",
        "    'ViT-B_32': get_b32_config(),\n",
        "    'ViT-L_16': get_l16_config(),\n",
        "    'ViT-L_32': get_l32_config(),\n",
        "    'ViT-H_14': get_h14_config(),\n",
        "    'R50-ViT-B_16': get_r50_b16_config(),\n",
        "    'testing': get_testing(),\n",
        "}"
      ],
      "metadata": {
        "id": "gGmB7W3utxEY"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# rearrange.py"
      ],
      "metadata": {
        "id": "lUzb7QX2tvfq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# pytorch imports\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import models, transforms, datasets\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.autograd import Variable\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import collections\n",
        "from shutil import copy\n",
        "from shutil import copytree, rmtree\n",
        "import random\n",
        "from tqdm import tqdm_notebook as tqdm\n",
        "import math\n",
        "import time\n",
        "# Helper method to split dataset into train and test folders\n",
        "def prepare_data(filepath, src, dest):\n",
        "    classes_images = defaultdict(list)\n",
        "    with open(filepath, 'r') as txt:\n",
        "        paths = [read.strip() for read in txt.readlines()]\n",
        "        for p in paths:\n",
        "            food = p.split('/')\n",
        "            classes_images[food[0]].append(food[1] + '.jpg')\n",
        "\n",
        "    for food in classes_images.keys():\n",
        "        print(\"\\nCopying images into \",food)\n",
        "        if not os.path.exists(os.path.join(dest,food)):\n",
        "            os.makedirs(os.path.join(dest,food))\n",
        "        for i in classes_images[food]:\n",
        "            copy(os.path.join(src,food,i), os.path.join(dest,food,i))\n",
        "    print(\"Copying Done!\")\n",
        "\n",
        "# # Prepare train dataset by copying images from food-101/images to food-101/train using the file train.txt\n",
        "\"\"\"\n",
        "print(\"Creating train data...\")\n",
        "META_PATH = dataset_path+\"/food-101/meta/\"\n",
        "IMG_PATH = dataset_path+\"/food-101/images/\"\n",
        "TRAIN_PATH = dataset_path+\"/food-101-processed/train/\"\n",
        "prepare_data(META_PATH+'train.txt', IMG_PATH, TRAIN_PATH)\n",
        "\"\"\"\n",
        "\n",
        "# # Prepare validation data by copying images from food-101/images to food-101/valid using the file test.txt\n",
        "\"\"\"\n",
        "print(\"Creating validation data...\")\n",
        "META_PATH = dataset_path+\"/food-101/meta/\"\n",
        "IMG_PATH = dataset_path+\"/food-101/images/\"\n",
        "TEST_PATH = dataset_path+\"/food-101-processed/test/\"\n",
        "prepare_data(META_PATH+'test.txt', IMG_PATH, TEST_PATH)\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "YWtmvRsetxhK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# train.py"
      ],
      "metadata": {
        "id": "yyy80GgutuF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install apex\n",
        "!pip install torch torchvision torchaudio\n",
        "!pip install apex.parallel"
      ],
      "metadata": {
        "id": "liKm8Osm4WsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import logging\n",
        "import argparse\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "from datetime import timedelta\n",
        "\n",
        "import torch\n",
        "import torch.distributed as dist\n",
        "\n",
        "from tqdm import tqdm\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# from apex import amp\n",
        "# from apex.parallel import DistributedDataParallel as DDP\n",
        "\n",
        "# from models.modeling import VisionTransformer, CONFIGS\n",
        "# from utils.scheduler import WarmupLinearSchedule, WarmupCosineSchedule\n",
        "# from utils.data_utils import get_loader\n",
        "# from utils.dist_util import get_world_size\n",
        "\n",
        "\n",
        "logger = logging.getLogger(__name__)\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    \"\"\"Computes and stores the average and current value\"\"\"\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "\n",
        "def simple_accuracy(preds, labels):\n",
        "    return (preds == labels).mean()\n",
        "\n",
        "\n",
        "def save_model(args, model):\n",
        "    model_to_save = model.module if hasattr(model, 'module') else model\n",
        "    model_checkpoint = os.path.join(args.output_dir, \"%s_checkpoint.bin\" % args.name)\n",
        "    torch.save(model_to_save.state_dict(), model_checkpoint)\n",
        "    logger.info(\"Saved model checkpoint to [DIR: %s]\", args.output_dir)\n",
        "\n",
        "\n",
        "def setup(args):\n",
        "    # Prepare model\n",
        "    config = CONFIGS[args.model_type]\n",
        "\n",
        "    num_classes = 101 if args.dataset == \"cifar10\" else 100\n",
        "\n",
        "    model = VisionTransformer(config, args.img_size, zero_head=True, num_classes=num_classes)\n",
        "    model.load_from(np.load(args.pretrained_dir))\n",
        "    model.to(args.device)\n",
        "    num_params = count_parameters(model)\n",
        "\n",
        "    logger.info(\"{}\".format(config))\n",
        "    logger.info(\"Training parameters %s\", args)\n",
        "    logger.info(\"Total Parameter: \\t%2.1fM\" % num_params)\n",
        "    print(num_params)\n",
        "    return args, model\n",
        "\n",
        "\n",
        "def count_parameters(model):\n",
        "    params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    return params/1000000\n",
        "\n",
        "\n",
        "def set_seed(args):\n",
        "    random.seed(args.seed)\n",
        "    np.random.seed(args.seed)\n",
        "    torch.manual_seed(args.seed)\n",
        "    if args.n_gpu > 0:\n",
        "        torch.cuda.manual_seed_all(args.seed)\n",
        "\n",
        "\n",
        "def valid(args, model, writer, test_loader, global_step):\n",
        "    # Validation!\n",
        "    eval_losses = AverageMeter()\n",
        "\n",
        "    logger.info(\"***** Running Validation *****\")\n",
        "    logger.info(\"  Num steps = %d\", len(test_loader))\n",
        "    logger.info(\"  Batch size = %d\", args.eval_batch_size)\n",
        "\n",
        "    model.eval()\n",
        "    all_preds, all_label = [], []\n",
        "    epoch_iterator = tqdm(test_loader,\n",
        "                          desc=\"Validating... (loss=X.X)\",\n",
        "                          bar_format=\"{l_bar}{r_bar}\",\n",
        "                          dynamic_ncols=True,\n",
        "                          disable=args.local_rank not in [-1, 0])\n",
        "    loss_fct = torch.nn.CrossEntropyLoss()\n",
        "    for step, batch in enumerate(epoch_iterator):\n",
        "        batch = tuple(t.to(args.device) for t in batch)\n",
        "        x, y = batch\n",
        "        with torch.no_grad():\n",
        "            logits = model(x)[0]\n",
        "            eval_loss = loss_fct(logits, y)\n",
        "            eval_losses.update(eval_loss.item())\n",
        "\n",
        "            preds = torch.argmax(logits, dim=-1)\n",
        "\n",
        "        if len(all_preds) == 0:\n",
        "            all_preds.append(preds.detach().cpu().numpy())\n",
        "            all_label.append(y.detach().cpu().numpy())\n",
        "        else:\n",
        "            all_preds[0] = np.append(\n",
        "                all_preds[0], preds.detach().cpu().numpy(), axis=0\n",
        "            )\n",
        "            all_label[0] = np.append(\n",
        "                all_label[0], y.detach().cpu().numpy(), axis=0\n",
        "            )\n",
        "        epoch_iterator.set_description(\"Validating... (loss=%2.5f)\" % eval_losses.val)\n",
        "\n",
        "    all_preds, all_label = all_preds[0], all_label[0]\n",
        "    accuracy = simple_accuracy(all_preds, all_label)\n",
        "\n",
        "    logger.info(\"\\n\")\n",
        "    logger.info(\"Validation Results\")\n",
        "    logger.info(\"Global Steps: %d\" % global_step)\n",
        "    logger.info(\"Valid Loss: %2.5f\" % eval_losses.avg)\n",
        "    logger.info(\"Valid Accuracy: %2.5f\" % accuracy)\n",
        "\n",
        "    writer.add_scalar(\"test/accuracy\", scalar_value=accuracy, global_step=global_step)\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "def train(args, model):\n",
        "    \"\"\" Train the model \"\"\"\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        os.makedirs(args.output_dir, exist_ok=True)\n",
        "        writer = SummaryWriter(log_dir=os.path.join(\"logs\", args.name))\n",
        "    args.train_batch_size = args.train_batch_size // args.gradient_accumulation_steps\n",
        "    # Prepare dataset\n",
        "    train_loader, test_loader = get_loader(args)\n",
        "    # Prepare optimizer and scheduler\n",
        "    optimizer = torch.optim.SGD(model.parameters(),\n",
        "                                lr=args.learning_rate,\n",
        "                                momentum=0.9,\n",
        "                                weight_decay=args.weight_decay)\n",
        "    t_total = args.num_steps\n",
        "    if args.decay_type == \"cosine\":\n",
        "        scheduler = WarmupCosineSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
        "    else:\n",
        "        scheduler = WarmupLinearSchedule(optimizer, warmup_steps=args.warmup_steps, t_total=t_total)\n",
        "\n",
        "    if args.fp16:\n",
        "        model, optimizer = amp.initialize(models=model,\n",
        "                                          optimizers=optimizer,\n",
        "                                          opt_level=args.fp16_opt_level)\n",
        "        amp._amp_state.loss_scalers[0]._loss_scale = 2**20\n",
        "\n",
        "    # Distributed training\n",
        "    if args.local_rank != -1:\n",
        "        model = DDP(model, message_size=250000000, gradient_predivide_factor=get_world_size())\n",
        "\n",
        "    # Train!\n",
        "    logger.info(\"***** Running training *****\")\n",
        "    logger.info(\"  Total optimization steps = %d\", args.num_steps)\n",
        "    logger.info(\"  Instantaneous batch size per GPU = %d\", args.train_batch_size)\n",
        "    logger.info(\"  Total train batch size (w. parallel, distributed & accumulation) = %d\",\n",
        "                args.train_batch_size * args.gradient_accumulation_steps * (\n",
        "                    torch.distributed.get_world_size() if args.local_rank != -1 else 1))\n",
        "    logger.info(\"  Gradient Accumulation steps = %d\", args.gradient_accumulation_steps)\n",
        "\n",
        "    model.zero_grad()\n",
        "    set_seed(args)  # Added here for reproducibility (even between python 2 and 3)\n",
        "    losses = AverageMeter()\n",
        "    global_step, best_acc = 0, 0\n",
        "    while True:\n",
        "        model.train()\n",
        "        epoch_iterator = tqdm(train_loader,\n",
        "                              desc=\"Training (X / X Steps) (loss=X.X)\",\n",
        "                              bar_format=\"{l_bar}{r_bar}\",\n",
        "                              dynamic_ncols=True,\n",
        "                              disable=args.local_rank not in [-1, 0])\n",
        "        for step, batch in enumerate(epoch_iterator):\n",
        "            batch = tuple(t.to(args.device) for t in batch)\n",
        "            x, y = batch\n",
        "            loss = model(x, y)\n",
        "\n",
        "            if args.gradient_accumulation_steps > 1:\n",
        "                loss = loss / args.gradient_accumulation_steps\n",
        "            if args.fp16:\n",
        "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
        "                    scaled_loss.backward()\n",
        "            else:\n",
        "                loss.backward()\n",
        "\n",
        "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
        "                losses.update(loss.item()*args.gradient_accumulation_steps)\n",
        "                if args.fp16:\n",
        "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
        "                else:\n",
        "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
        "                scheduler.step()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                global_step += 1\n",
        "                epoch_iterator.set_description(\n",
        "                    \"Training (%d / %d Steps) (loss=%2.5f)\" % (global_step, t_total, losses.val)\n",
        "                )\n",
        "                # load the best checkpoint if you want to perform only test\n",
        "                #model.load_state_dict(torch.load(\"best_checkpoint.bin\"))\n",
        "                if args.local_rank in [-1, 0]:\n",
        "                    writer.add_scalar(\"train/loss\", scalar_value=losses.val, global_step=global_step)\n",
        "                    writer.add_scalar(\"train/lr\", scalar_value=scheduler.get_lr()[0], global_step=global_step)\n",
        "                if global_step % args.eval_every == 0 and args.local_rank in [-1, 0]:\n",
        "                    accuracy = valid(args, model, writer, test_loader, global_step)\n",
        "                    if best_acc < accuracy:\n",
        "                        #save_model(args, model)\n",
        "                        #don't save any model at the time of only testing\n",
        "                        save_model(args, model)\n",
        "                        best_acc = accuracy\n",
        "                    model.train()\n",
        "\n",
        "                if global_step % t_total == 0:\n",
        "                    break\n",
        "        losses.reset()\n",
        "        if global_step % t_total == 0:\n",
        "            break\n",
        "\n",
        "    if args.local_rank in [-1, 0]:\n",
        "        writer.close()\n",
        "    logger.info(\"Best Accuracy: \\t%f\" % best_acc)\n",
        "    logger.info(\"End Training!\")\n",
        "\n",
        "\n",
        "# def main():\n",
        "#     parser = argparse.ArgumentParser()\n",
        "#     # Required parameters\n",
        "#     parser.add_argument(\"--name\", required=False,\n",
        "#                         help=\"Name of this run. Used for monitoring.\", default=\"best_checkpoint\")\n",
        "#     parser.add_argument(\"--dataset\", choices=[\"cifar10\", \"cifar100\"], default=\"cifar10\",\n",
        "#                         help=\"Which downstream task.\")\n",
        "#     parser.add_argument(\"--dataset_path\", default=\"root-path/food-101-processed\",help=\"root dataset path\")\n",
        "#     parser.add_argument(\"--model_type\", choices=[\"ViT-B_16\", \"ViT-B_32\", \"ViT-L_16\",\n",
        "#                                                  \"ViT-L_32\", \"ViT-H_14\", \"R50-ViT-B_16\"],default=\"ViT-B_16\",\n",
        "#                         help=\"Which variant to use.\")\n",
        "#     parser.add_argument(\"--pretrained_dir\", type=str, default=\"pretrained-model-path/ViT-B_16.npz\",\n",
        "#                         help=\"Where to search for pretrained ViT models.\")\n",
        "#     parser.add_argument(\"--output_dir\", default=\"output-path/\", type=str,\n",
        "#                         help=\"The output directory where checkpoints will be written.\")\n",
        "\n",
        "#     parser.add_argument(\"--img_size\", default=224, type=int,\n",
        "#                         help=\"Resolution size\")\n",
        "#     parser.add_argument(\"--train_batch_size\", default=32, type=int,\n",
        "#                         help=\"Total batch size for training.\")\n",
        "#     parser.add_argument(\"--eval_batch_size\", default=32, type=int,\n",
        "#                         help=\"Total batch size for eval.\")\n",
        "#     parser.add_argument(\"--eval_every\", default=10000, type=int,\n",
        "#                         help=\"Run prediction on validation set every so many steps.\"\n",
        "#                              \"Will always run one evaluation at the end of training.\")\n",
        "\n",
        "#     parser.add_argument(\"--learning_rate\", default=3e-2, type=float,\n",
        "#                         help=\"The initial learning rate for SGD.\")\n",
        "#     parser.add_argument(\"--weight_decay\", default=0, type=float,\n",
        "#                         help=\"Weight deay if we apply some.\")\n",
        "#     parser.add_argument(\"--num_steps\", default=100000, type=int,\n",
        "#                         help=\"Total number of training epochs to perform.\")\n",
        "#     parser.add_argument(\"--decay_type\", choices=[\"cosine\", \"linear\"], default=\"cosine\",\n",
        "#                         help=\"How to decay the learning rate.\")\n",
        "#     parser.add_argument(\"--warmup_steps\", default=500, type=int,\n",
        "#                         help=\"Step of training to perform learning rate warmup for.\")\n",
        "#     parser.add_argument(\"--max_grad_norm\", default=1.0, type=float,\n",
        "#                         help=\"Max gradient norm.\")\n",
        "\n",
        "#     parser.add_argument(\"--local_rank\", type=int, default=-1,\n",
        "#                         help=\"local_rank for distributed training on gpus\")\n",
        "#     parser.add_argument('--seed', type=int, default=42,\n",
        "#                         help=\"random seed for initialization\")\n",
        "#     parser.add_argument('--gradient_accumulation_steps', type=int, default=1,\n",
        "#                         help=\"Number of updates steps to accumulate before performing a backward/update pass.\")\n",
        "#     parser.add_argument('--fp16', action='store_true',\n",
        "#                         help=\"Whether to use 16-bit float precision instead of 32-bit\")\n",
        "#     parser.add_argument('--fp16_opt_level', type=str, default='O2',\n",
        "#                         help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
        "#                              \"See details at https://nvidia.github.io/apex/amp.html\")\n",
        "#     parser.add_argument('--loss_scale', type=float, default=0,\n",
        "#                         help=\"Loss scaling to improve fp16 numeric stability. Only used when fp16 set to True.\\n\"\n",
        "#                              \"0 (default value): dynamic loss scaling.\\n\"\n",
        "#                              \"Positive power of 2: static loss scaling value.\\n\")\n",
        "#     parser.add_argument(\"--phase\", choices=[\"train\", \"test\"], default=\"train\",help=\"Which operation you want to perform.\")\n",
        "#     args = parser.parse_args()"
      ],
      "metadata": {
        "id": "UtpjT2bstyBl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iigknr5s0qtA",
        "outputId": "80a93f38-64bc-4a08-bf94-55d61317fadd"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-16 14:41:53--  https://storage.googleapis.com/vit_models/imagenet21k/ViT-B_16.npz\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 173.194.202.128, 173.194.203.128, 74.125.199.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|173.194.202.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 412815506 (394M) [application/octet-stream]\n",
            "Saving to: ‘ViT-B_16.npz’\n",
            "\n",
            "ViT-B_16.npz        100%[===================>] 393.69M  27.0MB/s    in 17s     \n",
            "\n",
            "2023-07-16 14:42:11 (23.1 MB/s) - ‘ViT-B_16.npz’ saved [412815506/412815506]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "    import argparse\n",
        "    parser = argparse.ArgumentParser()\n",
        "    args = parser.parse_args([])\n",
        "    args.name = \"best_checkpoint\"\n",
        "    args.dataset = \"cifar10\"\n",
        "    args.dataset_path = \"/content/hateful_ViT1\"\n",
        "    args.model_type = \"ViT-B_16\"\n",
        "    args.pretrained_dir = \"/content/ViT-B_16.npz\"\n",
        "    args.output_dir = \"/content/\"\n",
        "    args.img_size = 256 # 224\n",
        "    args.train_batch_size = 16 #32\n",
        "    args.eval_batch_size = 32\n",
        "    args.eval_every = 1657 # 10000\n",
        "    args.learning_rate = 3e-2\n",
        "    args.weight_decay = 0\n",
        "    args.num_steps = 16570 # 100000\n",
        "    args.decay_type = \"cosine\"\n",
        "    args.warmup_steps = 500\n",
        "    args.max_grad_norm = 1.0\n",
        "    args.local_rank = -1\n",
        "    args.seed = 42\n",
        "    args.gradient_accumulation_steps = 1\n",
        "    args.fp16 = False\n",
        "    args.fp16_opt_level = 'O2'\n",
        "    args.loss_scale = 0\n",
        "    args.phase = \"train\"\n",
        "\n",
        "    # for multiple GPU we can directly assign the GPU index\n",
        "    # if args.local_rank == -1:\n",
        "    #     device = 1\n",
        "    #     args.n_gpu = torch.cuda.device_count()\n",
        "    # else:\n",
        "    #     torch.cuda.set_device(args.local_rank)\n",
        "    #     device = torch.device(\"cuda\", args.local_rank)\n",
        "    #     torch.distributed.init_process_group(backend='nccl',\n",
        "    #                                          timeout=timedelta(minutes=60))\n",
        "        # args.n_gpu = 1\n",
        "    args.n_gpu = 1\n",
        "    device = torch.device(\"cuda\")\n",
        "    args.device = device\n",
        "\n",
        "    logging.basicConfig(format='%(asctime)s - %(levelname)s - %(name)s - %(message)s',\n",
        "                        datefmt='%m/%d/%Y %H:%M:%S',\n",
        "                        level=logging.INFO if args.local_rank in [-1, 0] else logging.WARN)\n",
        "    logger.warning(\"Process rank: %s, device: %s, n_gpu: %s, distributed training: %s, 16-bits training: %s\" %\n",
        "                   (args.local_rank, args.device, args.n_gpu, bool(args.local_rank != -1), args.fp16))\n",
        "    set_seed(args)\n",
        "    args, model = setup(args)\n",
        "    # Training\n",
        "    train(args, model)\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "fJKYKqtp0qbs",
        "outputId": "2a7af613-4ac9-4d59-de5c-bb82e96c7292"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training (1657 / 16570 Steps) (loss=0.61433): 100%|| 1656/1657 [18:01<00:00,  1.53it/s]\n",
            "Validating... (loss=X.X):   0%|| 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.57475):   0%|| 0/63 [00:01<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.57475):   2%|| 1/63 [00:01<01:24,  1.37s/it]\u001b[A\n",
            "Validating... (loss=0.65501):   2%|| 1/63 [00:01<01:24,  1.37s/it]\u001b[A\n",
            "Validating... (loss=0.65501):   3%|| 2/63 [00:01<00:52,  1.17it/s]\u001b[A\n",
            "Validating... (loss=0.58854):   3%|| 2/63 [00:02<00:52,  1.17it/s]\u001b[A\n",
            "Validating... (loss=0.58854):   5%|| 3/63 [00:02<00:41,  1.46it/s]\u001b[A\n",
            "Validating... (loss=0.55854):   5%|| 3/63 [00:02<00:41,  1.46it/s]\u001b[A\n",
            "Validating... (loss=0.55854):   6%|| 4/63 [00:02<00:35,  1.68it/s]\u001b[A\n",
            "Validating... (loss=0.65503):   6%|| 4/63 [00:03<00:35,  1.68it/s]\u001b[A\n",
            "Validating... (loss=0.65503):   8%|| 5/63 [00:03<00:32,  1.80it/s]\u001b[A\n",
            "Validating... (loss=0.59840):   8%|| 5/63 [00:03<00:32,  1.80it/s]\u001b[A\n",
            "Validating... (loss=0.59840):  10%|| 6/63 [00:03<00:30,  1.89it/s]\u001b[A\n",
            "Validating... (loss=0.60534):  10%|| 6/63 [00:04<00:30,  1.89it/s]\u001b[A\n",
            "Validating... (loss=0.60534):  11%|| 7/63 [00:04<00:28,  1.98it/s]\u001b[A\n",
            "Validating... (loss=0.62570):  11%|| 7/63 [00:04<00:28,  1.98it/s]\u001b[A\n",
            "Validating... (loss=0.62570):  13%|| 8/63 [00:04<00:27,  2.03it/s]\u001b[A\n",
            "Validating... (loss=0.58537):  13%|| 8/63 [00:05<00:27,  2.03it/s]\u001b[A\n",
            "Validating... (loss=0.58537):  14%|| 9/63 [00:05<00:26,  2.06it/s]\u001b[A\n",
            "Validating... (loss=0.58801):  14%|| 9/63 [00:05<00:26,  2.06it/s]\u001b[A\n",
            "Validating... (loss=0.58801):  16%|| 10/63 [00:05<00:25,  2.09it/s]\u001b[A\n",
            "Validating... (loss=0.59940):  16%|| 10/63 [00:06<00:25,  2.09it/s]\u001b[A\n",
            "Validating... (loss=0.59940):  17%|| 11/63 [00:06<00:24,  2.09it/s]\u001b[A\n",
            "Validating... (loss=0.61852):  17%|| 11/63 [00:06<00:24,  2.09it/s]\u001b[A\n",
            "Validating... (loss=0.61852):  19%|| 12/63 [00:06<00:24,  2.10it/s]\u001b[A\n",
            "Validating... (loss=0.63847):  19%|| 12/63 [00:07<00:24,  2.10it/s]\u001b[A\n",
            "Validating... (loss=0.63847):  21%|| 13/63 [00:07<00:23,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.59802):  21%|| 13/63 [00:07<00:23,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.59802):  22%|| 14/63 [00:07<00:23,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.63880):  22%|| 14/63 [00:07<00:23,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.63880):  24%|| 15/63 [00:07<00:22,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.57663):  24%|| 15/63 [00:08<00:22,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.57663):  25%|| 16/63 [00:08<00:21,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.61865):  25%|| 16/63 [00:08<00:21,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.61865):  27%|| 17/63 [00:08<00:21,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.58744):  27%|| 17/63 [00:09<00:21,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.58744):  29%|| 18/63 [00:09<00:20,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.58157):  29%|| 18/63 [00:09<00:20,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.58157):  30%|| 19/63 [00:09<00:20,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.59919):  30%|| 19/63 [00:10<00:20,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.59919):  32%|| 20/63 [00:10<00:19,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.63309):  32%|| 20/63 [00:10<00:19,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.63309):  33%|| 21/63 [00:10<00:19,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.58415):  33%|| 21/63 [00:11<00:19,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.58415):  35%|| 22/63 [00:11<00:19,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.68594):  35%|| 22/63 [00:11<00:19,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.68594):  37%|| 23/63 [00:11<00:18,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.59276):  37%|| 23/63 [00:12<00:18,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.59276):  38%|| 24/63 [00:12<00:18,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.57964):  38%|| 24/63 [00:12<00:18,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.57964):  40%|| 25/63 [00:12<00:17,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.56292):  40%|| 25/63 [00:13<00:17,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.56292):  41%|| 26/63 [00:13<00:17,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.61268):  41%|| 26/63 [00:13<00:17,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.61268):  43%|| 27/63 [00:13<00:16,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.59727):  43%|| 27/63 [00:13<00:16,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.59727):  44%|| 28/63 [00:13<00:16,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.58845):  44%|| 28/63 [00:14<00:16,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.58845):  46%|| 29/63 [00:14<00:15,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.60529):  46%|| 29/63 [00:14<00:15,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.60529):  48%|| 30/63 [00:14<00:15,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.64960):  48%|| 30/63 [00:15<00:15,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.64960):  49%|| 31/63 [00:15<00:14,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.58653):  49%|| 31/63 [00:15<00:14,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.58653):  51%|| 32/63 [00:15<00:14,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.58475):  51%|| 32/63 [00:16<00:14,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.58475):  52%|| 33/63 [00:16<00:13,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.62906):  52%|| 33/63 [00:16<00:13,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.62906):  54%|| 34/63 [00:16<00:13,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.60836):  54%|| 34/63 [00:17<00:13,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.60836):  56%|| 35/63 [00:17<00:12,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.60200):  56%|| 35/63 [00:17<00:12,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.60200):  57%|| 36/63 [00:17<00:12,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.67454):  57%|| 36/63 [00:18<00:12,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.67454):  59%|| 37/63 [00:18<00:12,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.57890):  59%|| 37/63 [00:18<00:12,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.57890):  60%|| 38/63 [00:18<00:11,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.66425):  60%|| 38/63 [00:19<00:11,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.66425):  62%|| 39/63 [00:19<00:11,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.79488):  62%|| 39/63 [00:19<00:11,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.79488):  63%|| 40/63 [00:19<00:10,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.75787):  63%|| 40/63 [00:19<00:10,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.75787):  65%|| 41/63 [00:19<00:10,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.73561):  65%|| 41/63 [00:20<00:10,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.73561):  67%|| 42/63 [00:20<00:09,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.78281):  67%|| 42/63 [00:20<00:09,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.78281):  68%|| 43/63 [00:20<00:09,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.74767):  68%|| 43/63 [00:21<00:09,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.74767):  70%|| 44/63 [00:21<00:08,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.75026):  70%|| 44/63 [00:21<00:08,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.75026):  71%|| 45/63 [00:21<00:08,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.82436):  71%|| 45/63 [00:22<00:08,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.82436):  73%|| 46/63 [00:22<00:07,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.77771):  73%|| 46/63 [00:22<00:07,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.77771):  75%|| 47/63 [00:22<00:07,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.76471):  75%|| 47/63 [00:23<00:07,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.76471):  76%|| 48/63 [00:23<00:06,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.77827):  76%|| 48/63 [00:23<00:06,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.77827):  78%|| 49/63 [00:23<00:06,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.81853):  78%|| 49/63 [00:24<00:06,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.81853):  79%|| 50/63 [00:24<00:06,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.78906):  79%|| 50/63 [00:24<00:06,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.78906):  81%|| 51/63 [00:24<00:05,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.80871):  81%|| 51/63 [00:25<00:05,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.80871):  83%|| 52/63 [00:25<00:05,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.83065):  83%|| 52/63 [00:25<00:05,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.83065):  84%|| 53/63 [00:25<00:04,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.81538):  84%|| 53/63 [00:25<00:04,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.81538):  86%|| 54/63 [00:26<00:04,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.81517):  86%|| 54/63 [00:26<00:04,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.81517):  87%|| 55/63 [00:26<00:03,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.77093):  87%|| 55/63 [00:26<00:03,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.77093):  89%|| 56/63 [00:26<00:03,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.77061):  89%|| 56/63 [00:27<00:03,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.77061):  90%|| 57/63 [00:27<00:02,  2.20it/s]\u001b[A\n",
            "Validating... (loss=0.79160):  90%|| 57/63 [00:27<00:02,  2.20it/s]\u001b[A\n",
            "Validating... (loss=0.79160):  92%|| 58/63 [00:27<00:02,  2.20it/s]\u001b[A\n",
            "Validating... (loss=0.81052):  92%|| 58/63 [00:28<00:02,  2.20it/s]\u001b[A\n",
            "Validating... (loss=0.81052):  94%|| 59/63 [00:28<00:01,  2.21it/s]\u001b[A\n",
            "Validating... (loss=0.72644):  94%|| 59/63 [00:28<00:01,  2.21it/s]\u001b[A\n",
            "Validating... (loss=0.72644):  95%|| 60/63 [00:28<00:01,  2.20it/s]\u001b[A\n",
            "Validating... (loss=0.78066):  95%|| 60/63 [00:29<00:01,  2.20it/s]\u001b[A\n",
            "Validating... (loss=0.78066):  97%|| 61/63 [00:29<00:00,  2.20it/s]\u001b[A\n",
            "Validating... (loss=0.77972):  97%|| 61/63 [00:29<00:00,  2.20it/s]\u001b[A\n",
            "Validating... (loss=0.77972):  98%|| 62/63 [00:29<00:00,  2.21it/s]\u001b[A\n",
            "Validating... (loss=0.81709):  98%|| 62/63 [00:29<00:00,  2.21it/s]\u001b[A\n",
            "Validating... (loss=0.81709): 100%|| 63/63 [00:29<00:00,  2.10it/s]\n",
            "Training (1657 / 16570 Steps) (loss=0.61433): 100%|| 1657/1657 [18:32<00:00,  1.49it/s]\n",
            "Training (3314 / 16570 Steps) (loss=0.70581): 100%|| 1656/1657 [18:07<00:00,  1.53it/s]\n",
            "Validating... (loss=X.X):   0%|| 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.52227):   0%|| 0/63 [00:01<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.52227):   2%|| 1/63 [00:01<01:08,  1.11s/it]\u001b[A\n",
            "Validating... (loss=0.65758):   2%|| 1/63 [00:01<01:08,  1.11s/it]\u001b[A\n",
            "Validating... (loss=0.65758):   3%|| 2/63 [00:01<00:45,  1.35it/s]\u001b[A\n",
            "Validating... (loss=0.52830):   3%|| 2/63 [00:02<00:45,  1.35it/s]\u001b[A\n",
            "Validating... (loss=0.52830):   5%|| 3/63 [00:02<00:36,  1.64it/s]\u001b[A\n",
            "Validating... (loss=0.49315):   5%|| 3/63 [00:02<00:36,  1.64it/s]\u001b[A\n",
            "Validating... (loss=0.49315):   6%|| 4/63 [00:02<00:32,  1.82it/s]\u001b[A\n",
            "Validating... (loss=0.59746):   6%|| 4/63 [00:02<00:32,  1.82it/s]\u001b[A\n",
            "Validating... (loss=0.59746):   8%|| 5/63 [00:02<00:30,  1.93it/s]\u001b[A\n",
            "Validating... (loss=0.49913):   8%|| 5/63 [00:03<00:30,  1.93it/s]\u001b[A\n",
            "Validating... (loss=0.49913):  10%|| 6/63 [00:03<00:28,  2.01it/s]\u001b[A\n",
            "Validating... (loss=0.54517):  10%|| 6/63 [00:03<00:28,  2.01it/s]\u001b[A\n",
            "Validating... (loss=0.54517):  11%|| 7/63 [00:03<00:27,  2.05it/s]\u001b[A\n",
            "Validating... (loss=0.58156):  11%|| 7/63 [00:04<00:27,  2.05it/s]\u001b[A\n",
            "Validating... (loss=0.58156):  13%|| 8/63 [00:04<00:26,  2.11it/s]\u001b[A\n",
            "Validating... (loss=0.52119):  13%|| 8/63 [00:04<00:26,  2.11it/s]\u001b[A\n",
            "Validating... (loss=0.52119):  14%|| 9/63 [00:04<00:25,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.53326):  14%|| 9/63 [00:05<00:25,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.53326):  16%|| 10/63 [00:05<00:24,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.53141):  16%|| 10/63 [00:05<00:24,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.53141):  17%|| 11/63 [00:05<00:24,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.57607):  17%|| 11/63 [00:06<00:24,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.57607):  19%|| 12/63 [00:06<00:23,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.54021):  19%|| 12/63 [00:06<00:23,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.54021):  21%|| 13/63 [00:06<00:23,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.60050):  21%|| 13/63 [00:07<00:23,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.60050):  22%|| 14/63 [00:07<00:22,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.52614):  22%|| 14/63 [00:07<00:22,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.52614):  24%|| 15/63 [00:07<00:22,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.50115):  24%|| 15/63 [00:07<00:22,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.50115):  25%|| 16/63 [00:08<00:21,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.56337):  25%|| 16/63 [00:08<00:21,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.56337):  27%|| 17/63 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.51464):  27%|| 17/63 [00:08<00:21,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.51464):  29%|| 18/63 [00:08<00:20,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.49429):  29%|| 18/63 [00:09<00:20,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.49429):  30%|| 19/63 [00:09<00:20,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.52011):  30%|| 19/63 [00:09<00:20,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.52011):  32%|| 20/63 [00:09<00:19,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.61466):  32%|| 20/63 [00:10<00:19,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.61466):  33%|| 21/63 [00:10<00:19,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.55680):  33%|| 21/63 [00:10<00:19,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.55680):  35%|| 22/63 [00:10<00:18,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.63141):  35%|| 22/63 [00:11<00:18,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.63141):  37%|| 23/63 [00:11<00:18,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.53174):  37%|| 23/63 [00:11<00:18,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.53174):  38%|| 24/63 [00:11<00:18,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.46714):  38%|| 24/63 [00:12<00:18,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.46714):  40%|| 25/63 [00:12<00:17,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.51578):  40%|| 25/63 [00:12<00:17,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.51578):  41%|| 26/63 [00:12<00:17,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.54784):  41%|| 26/63 [00:13<00:17,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.54784):  43%|| 27/63 [00:13<00:16,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.52994):  43%|| 27/63 [00:13<00:16,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.52994):  44%|| 28/63 [00:13<00:16,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.45853):  44%|| 28/63 [00:14<00:16,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.45853):  46%|| 29/63 [00:14<00:15,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.51331):  46%|| 29/63 [00:14<00:15,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.51331):  48%|| 30/63 [00:14<00:15,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.56147):  48%|| 30/63 [00:14<00:15,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.56147):  49%|| 31/63 [00:14<00:14,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.51537):  49%|| 31/63 [00:15<00:14,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.51537):  51%|| 32/63 [00:15<00:14,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.51409):  51%|| 32/63 [00:15<00:14,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.51409):  52%|| 33/63 [00:15<00:13,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.55621):  52%|| 33/63 [00:16<00:13,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.55621):  54%|| 34/63 [00:16<00:13,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.54135):  54%|| 34/63 [00:16<00:13,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.54135):  56%|| 35/63 [00:16<00:12,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.49609):  56%|| 35/63 [00:17<00:12,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.49609):  57%|| 36/63 [00:17<00:12,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.59289):  57%|| 36/63 [00:17<00:12,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.59289):  59%|| 37/63 [00:17<00:11,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.45902):  59%|| 37/63 [00:18<00:11,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.45902):  60%|| 38/63 [00:18<00:11,  2.20it/s]\u001b[A\n",
            "Validating... (loss=0.58989):  60%|| 38/63 [00:18<00:11,  2.20it/s]\u001b[A\n",
            "Validating... (loss=0.58989):  62%|| 39/63 [00:18<00:10,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.83884):  62%|| 39/63 [00:19<00:10,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.83884):  63%|| 40/63 [00:19<00:10,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.83043):  63%|| 40/63 [00:19<00:10,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.83043):  65%|| 41/63 [00:19<00:10,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.85951):  65%|| 41/63 [00:19<00:10,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.85951):  67%|| 42/63 [00:19<00:09,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.88418):  67%|| 42/63 [00:20<00:09,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.88418):  68%|| 43/63 [00:20<00:09,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.87943):  68%|| 43/63 [00:20<00:09,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.87943):  70%|| 44/63 [00:20<00:08,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.83898):  70%|| 44/63 [00:21<00:08,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.83898):  71%|| 45/63 [00:21<00:08,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.93496):  71%|| 45/63 [00:21<00:08,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.93496):  73%|| 46/63 [00:21<00:07,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.89239):  73%|| 46/63 [00:22<00:07,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.89239):  75%|| 47/63 [00:22<00:07,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.88249):  75%|| 47/63 [00:22<00:07,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.88249):  76%|| 48/63 [00:22<00:06,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.91996):  76%|| 48/63 [00:23<00:06,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.91996):  78%|| 49/63 [00:23<00:06,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.03104):  78%|| 49/63 [00:23<00:06,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.03104):  79%|| 50/63 [00:23<00:06,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.91009):  79%|| 50/63 [00:24<00:06,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.91009):  81%|| 51/63 [00:24<00:05,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.93370):  81%|| 51/63 [00:24<00:05,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.93370):  83%|| 52/63 [00:24<00:05,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.91288):  83%|| 52/63 [00:25<00:05,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.91288):  84%|| 53/63 [00:25<00:04,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.92580):  84%|| 53/63 [00:25<00:04,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.92580):  86%|| 54/63 [00:25<00:04,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.92776):  86%|| 54/63 [00:26<00:04,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.92776):  87%|| 55/63 [00:26<00:03,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.90562):  87%|| 55/63 [00:26<00:03,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.90562):  89%|| 56/63 [00:26<00:03,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.84948):  89%|| 56/63 [00:26<00:03,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.84948):  90%|| 57/63 [00:26<00:02,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.81087):  90%|| 57/63 [00:27<00:02,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.81087):  92%|| 58/63 [00:27<00:02,  2.19it/s]\u001b[A\n",
            "Validating... (loss=1.03382):  92%|| 58/63 [00:27<00:02,  2.19it/s]\u001b[A\n",
            "Validating... (loss=1.03382):  94%|| 59/63 [00:27<00:01,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.79083):  94%|| 59/63 [00:28<00:01,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.79083):  95%|| 60/63 [00:28<00:01,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.87592):  95%|| 60/63 [00:28<00:01,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.87592):  97%|| 61/63 [00:28<00:00,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.83160):  97%|| 61/63 [00:29<00:00,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.83160):  98%|| 62/63 [00:29<00:00,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.96485):  98%|| 62/63 [00:29<00:00,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.96485): 100%|| 63/63 [00:29<00:00,  2.13it/s]\n",
            "Training (3314 / 16570 Steps) (loss=0.70581): 100%|| 1657/1657 [18:43<00:00,  1.47it/s]\n",
            "Training (4971 / 16570 Steps) (loss=0.88516): 100%|| 1656/1657 [18:09<00:00,  1.53it/s]\n",
            "Validating... (loss=X.X):   0%|| 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.42284):   0%|| 0/63 [00:01<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.42284):   2%|| 1/63 [00:01<01:12,  1.18s/it]\u001b[A\n",
            "Validating... (loss=0.47137):   2%|| 1/63 [00:01<01:12,  1.18s/it]\u001b[A\n",
            "Validating... (loss=0.47137):   3%|| 2/63 [00:01<00:45,  1.33it/s]\u001b[A\n",
            "Validating... (loss=0.44599):   3%|| 2/63 [00:02<00:45,  1.33it/s]\u001b[A\n",
            "Validating... (loss=0.44599):   5%|| 3/63 [00:02<00:37,  1.61it/s]\u001b[A\n",
            "Validating... (loss=0.39102):   5%|| 3/63 [00:02<00:37,  1.61it/s]\u001b[A\n",
            "Validating... (loss=0.39102):   6%|| 4/63 [00:02<00:32,  1.80it/s]\u001b[A\n",
            "Validating... (loss=0.43347):   6%|| 4/63 [00:03<00:32,  1.80it/s]\u001b[A\n",
            "Validating... (loss=0.43347):   8%|| 5/63 [00:03<00:30,  1.90it/s]\u001b[A\n",
            "Validating... (loss=0.37720):   8%|| 5/63 [00:03<00:30,  1.90it/s]\u001b[A\n",
            "Validating... (loss=0.37720):  10%|| 6/63 [00:03<00:28,  2.01it/s]\u001b[A\n",
            "Validating... (loss=0.42358):  10%|| 6/63 [00:03<00:28,  2.01it/s]\u001b[A\n",
            "Validating... (loss=0.42358):  11%|| 7/63 [00:03<00:27,  2.05it/s]\u001b[A\n",
            "Validating... (loss=0.38791):  11%|| 7/63 [00:04<00:27,  2.05it/s]\u001b[A\n",
            "Validating... (loss=0.38791):  13%|| 8/63 [00:04<00:26,  2.10it/s]\u001b[A\n",
            "Validating... (loss=0.35344):  13%|| 8/63 [00:04<00:26,  2.10it/s]\u001b[A\n",
            "Validating... (loss=0.35344):  14%|| 9/63 [00:04<00:25,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.32716):  14%|| 9/63 [00:05<00:25,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.32716):  16%|| 10/63 [00:05<00:24,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.33061):  16%|| 10/63 [00:05<00:24,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.33061):  17%|| 11/63 [00:05<00:24,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.40428):  17%|| 11/63 [00:06<00:24,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.40428):  19%|| 12/63 [00:06<00:23,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.37670):  19%|| 12/63 [00:06<00:23,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.37670):  21%|| 13/63 [00:06<00:23,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.48957):  21%|| 13/63 [00:07<00:23,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.48957):  22%|| 14/63 [00:07<00:22,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.38936):  22%|| 14/63 [00:07<00:22,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.38936):  24%|| 15/63 [00:07<00:22,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.34930):  24%|| 15/63 [00:08<00:22,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.34930):  25%|| 16/63 [00:08<00:21,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.39630):  25%|| 16/63 [00:08<00:21,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.39630):  27%|| 17/63 [00:08<00:21,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.39924):  27%|| 17/63 [00:08<00:21,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.39924):  29%|| 18/63 [00:09<00:20,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.34709):  29%|| 18/63 [00:09<00:20,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.34709):  30%|| 19/63 [00:09<00:20,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.44774):  30%|| 19/63 [00:09<00:20,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.44774):  32%|| 20/63 [00:09<00:19,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.54875):  32%|| 20/63 [00:10<00:19,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.54875):  33%|| 21/63 [00:10<00:19,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.34833):  33%|| 21/63 [00:10<00:19,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.34833):  35%|| 22/63 [00:10<00:18,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.46763):  35%|| 22/63 [00:11<00:18,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.46763):  37%|| 23/63 [00:11<00:18,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.43090):  37%|| 23/63 [00:11<00:18,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.43090):  38%|| 24/63 [00:11<00:17,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.31087):  38%|| 24/63 [00:12<00:17,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.31087):  40%|| 25/63 [00:12<00:17,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.34540):  40%|| 25/63 [00:12<00:17,  2.19it/s]\u001b[A\n",
            "Validating... (loss=0.34540):  41%|| 26/63 [00:12<00:17,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.38643):  41%|| 26/63 [00:13<00:17,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.38643):  43%|| 27/63 [00:13<00:16,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.28660):  43%|| 27/63 [00:13<00:16,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.28660):  44%|| 28/63 [00:13<00:16,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.33664):  44%|| 28/63 [00:14<00:16,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.33664):  46%|| 29/63 [00:14<00:15,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.33006):  46%|| 29/63 [00:14<00:15,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.33006):  48%|| 30/63 [00:14<00:15,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.39258):  48%|| 30/63 [00:15<00:15,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.39258):  49%|| 31/63 [00:15<00:14,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.32816):  49%|| 31/63 [00:15<00:14,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.32816):  51%|| 32/63 [00:15<00:14,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.39230):  51%|| 32/63 [00:15<00:14,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.39230):  52%|| 33/63 [00:15<00:13,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.35073):  52%|| 33/63 [00:16<00:13,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.35073):  54%|| 34/63 [00:16<00:13,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.34813):  54%|| 34/63 [00:16<00:13,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.34813):  56%|| 35/63 [00:16<00:12,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.33838):  56%|| 35/63 [00:17<00:12,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.33838):  57%|| 36/63 [00:17<00:12,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.42207):  57%|| 36/63 [00:17<00:12,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.42207):  59%|| 37/63 [00:17<00:11,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.37765):  59%|| 37/63 [00:18<00:11,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.37765):  60%|| 38/63 [00:18<00:11,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.41943):  60%|| 38/63 [00:18<00:11,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.41943):  62%|| 39/63 [00:18<00:11,  2.18it/s]\u001b[A\n",
            "Validating... (loss=1.01107):  62%|| 39/63 [00:19<00:11,  2.18it/s]\u001b[A\n",
            "Validating... (loss=1.01107):  63%|| 40/63 [00:19<00:10,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.08644):  63%|| 40/63 [00:19<00:10,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.08644):  65%|| 41/63 [00:19<00:10,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.23001):  65%|| 41/63 [00:20<00:10,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.23001):  67%|| 42/63 [00:20<00:09,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.23507):  67%|| 42/63 [00:20<00:09,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.23507):  68%|| 43/63 [00:20<00:09,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.08734):  68%|| 43/63 [00:21<00:09,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.08734):  70%|| 44/63 [00:21<00:08,  2.14it/s]\u001b[A\n",
            "Validating... (loss=1.11936):  70%|| 44/63 [00:21<00:08,  2.14it/s]\u001b[A\n",
            "Validating... (loss=1.11936):  71%|| 45/63 [00:21<00:08,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.15913):  71%|| 45/63 [00:21<00:08,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.15913):  73%|| 46/63 [00:21<00:07,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.24541):  73%|| 46/63 [00:22<00:07,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.24541):  75%|| 47/63 [00:22<00:07,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.07396):  75%|| 47/63 [00:22<00:07,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.07396):  76%|| 48/63 [00:22<00:06,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.28012):  76%|| 48/63 [00:23<00:06,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.28012):  78%|| 49/63 [00:23<00:06,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.27155):  78%|| 49/63 [00:23<00:06,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.27155):  79%|| 50/63 [00:23<00:06,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.18991):  79%|| 50/63 [00:24<00:06,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.18991):  81%|| 51/63 [00:24<00:05,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.22073):  81%|| 51/63 [00:24<00:05,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.22073):  83%|| 52/63 [00:24<00:05,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.06853):  83%|| 52/63 [00:25<00:05,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.06853):  84%|| 53/63 [00:25<00:04,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.28171):  84%|| 53/63 [00:25<00:04,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.28171):  86%|| 54/63 [00:25<00:04,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.43174):  86%|| 54/63 [00:26<00:04,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.43174):  87%|| 55/63 [00:26<00:03,  2.19it/s]\u001b[A\n",
            "Validating... (loss=1.16615):  87%|| 55/63 [00:26<00:03,  2.19it/s]\u001b[A\n",
            "Validating... (loss=1.16615):  89%|| 56/63 [00:26<00:03,  2.18it/s]\u001b[A\n",
            "Validating... (loss=1.15974):  89%|| 56/63 [00:26<00:03,  2.18it/s]\u001b[A\n",
            "Validating... (loss=1.15974):  90%|| 57/63 [00:27<00:02,  2.19it/s]\u001b[A\n",
            "Validating... (loss=1.23726):  90%|| 57/63 [00:27<00:02,  2.19it/s]\u001b[A\n",
            "Validating... (loss=1.23726):  92%|| 58/63 [00:27<00:02,  2.20it/s]\u001b[A\n",
            "Validating... (loss=1.35949):  92%|| 58/63 [00:27<00:02,  2.20it/s]\u001b[A\n",
            "Validating... (loss=1.35949):  94%|| 59/63 [00:27<00:01,  2.21it/s]\u001b[A\n",
            "Validating... (loss=1.03680):  94%|| 59/63 [00:28<00:01,  2.21it/s]\u001b[A\n",
            "Validating... (loss=1.03680):  95%|| 60/63 [00:28<00:01,  2.21it/s]\u001b[A\n",
            "Validating... (loss=1.02652):  95%|| 60/63 [00:28<00:01,  2.21it/s]\u001b[A\n",
            "Validating... (loss=1.02652):  97%|| 61/63 [00:28<00:00,  2.20it/s]\u001b[A\n",
            "Validating... (loss=1.21331):  97%|| 61/63 [00:29<00:00,  2.20it/s]\u001b[A\n",
            "Validating... (loss=1.21331):  98%|| 62/63 [00:29<00:00,  2.20it/s]\u001b[A\n",
            "Validating... (loss=1.25199):  98%|| 62/63 [00:29<00:00,  2.20it/s]\u001b[A\n",
            "Validating... (loss=1.25199): 100%|| 63/63 [00:29<00:00,  2.13it/s]\n",
            "Training (4971 / 16570 Steps) (loss=0.88516): 100%|| 1657/1657 [18:39<00:00,  1.48it/s]\n",
            "Training (6628 / 16570 Steps) (loss=0.42211): 100%|| 1656/1657 [18:08<00:00,  1.54it/s]\n",
            "Validating... (loss=X.X):   0%|| 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.39002):   0%|| 0/63 [00:00<?, ?it/s]\u001b[A\n",
            "Validating... (loss=0.39002):   2%|| 1/63 [00:00<01:01,  1.01it/s]\u001b[A\n",
            "Validating... (loss=0.49952):   2%|| 1/63 [00:01<01:01,  1.01it/s]\u001b[A\n",
            "Validating... (loss=0.49952):   3%|| 2/63 [00:01<00:42,  1.45it/s]\u001b[A\n",
            "Validating... (loss=0.52408):   3%|| 2/63 [00:01<00:42,  1.45it/s]\u001b[A\n",
            "Validating... (loss=0.52408):   5%|| 3/63 [00:01<00:35,  1.70it/s]\u001b[A\n",
            "Validating... (loss=0.42700):   5%|| 3/63 [00:02<00:35,  1.70it/s]\u001b[A\n",
            "Validating... (loss=0.42700):   6%|| 4/63 [00:02<00:31,  1.85it/s]\u001b[A\n",
            "Validating... (loss=0.45666):   6%|| 4/63 [00:02<00:31,  1.85it/s]\u001b[A\n",
            "Validating... (loss=0.45666):   8%|| 5/63 [00:02<00:29,  1.96it/s]\u001b[A\n",
            "Validating... (loss=0.45317):   8%|| 5/63 [00:03<00:29,  1.96it/s]\u001b[A\n",
            "Validating... (loss=0.45317):  10%|| 6/63 [00:03<00:28,  2.03it/s]\u001b[A\n",
            "Validating... (loss=0.48015):  10%|| 6/63 [00:03<00:28,  2.03it/s]\u001b[A\n",
            "Validating... (loss=0.48015):  11%|| 7/63 [00:03<00:26,  2.08it/s]\u001b[A\n",
            "Validating... (loss=0.47705):  11%|| 7/63 [00:04<00:26,  2.08it/s]\u001b[A\n",
            "Validating... (loss=0.47705):  13%|| 8/63 [00:04<00:26,  2.09it/s]\u001b[A\n",
            "Validating... (loss=0.43570):  13%|| 8/63 [00:04<00:26,  2.09it/s]\u001b[A\n",
            "Validating... (loss=0.43570):  14%|| 9/63 [00:04<00:25,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.46971):  14%|| 9/63 [00:05<00:25,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.46971):  16%|| 10/63 [00:05<00:25,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.38690):  16%|| 10/63 [00:05<00:25,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.38690):  17%|| 11/63 [00:05<00:24,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.40643):  17%|| 11/63 [00:06<00:24,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.40643):  19%|| 12/63 [00:06<00:23,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.52571):  19%|| 12/63 [00:06<00:23,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.52571):  21%|| 13/63 [00:06<00:23,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.43874):  21%|| 13/63 [00:07<00:23,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.43874):  22%|| 14/63 [00:07<00:23,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.47083):  22%|| 14/63 [00:07<00:23,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.47083):  24%|| 15/63 [00:07<00:22,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.36072):  24%|| 15/63 [00:07<00:22,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.36072):  25%|| 16/63 [00:07<00:21,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.51410):  25%|| 16/63 [00:08<00:21,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.51410):  27%|| 17/63 [00:08<00:21,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.45338):  27%|| 17/63 [00:08<00:21,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.45338):  29%|| 18/63 [00:08<00:20,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.39034):  29%|| 18/63 [00:09<00:20,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.39034):  30%|| 19/63 [00:09<00:20,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.43408):  30%|| 19/63 [00:09<00:20,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.43408):  32%|| 20/63 [00:09<00:20,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.55512):  32%|| 20/63 [00:10<00:20,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.55512):  33%|| 21/63 [00:10<00:19,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.49721):  33%|| 21/63 [00:10<00:19,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.49721):  35%|| 22/63 [00:10<00:19,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.60435):  35%|| 22/63 [00:11<00:19,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.60435):  37%|| 23/63 [00:11<00:18,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.46891):  37%|| 23/63 [00:11<00:18,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.46891):  38%|| 24/63 [00:11<00:18,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.41052):  38%|| 24/63 [00:12<00:18,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.41052):  40%|| 25/63 [00:12<00:17,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.41906):  40%|| 25/63 [00:12<00:17,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.41906):  41%|| 26/63 [00:12<00:16,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.45350):  41%|| 26/63 [00:13<00:16,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.45350):  43%|| 27/63 [00:13<00:16,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.32904):  43%|| 27/63 [00:13<00:16,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.32904):  44%|| 28/63 [00:13<00:16,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.40332):  44%|| 28/63 [00:13<00:16,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.40332):  46%|| 29/63 [00:13<00:15,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.37416):  46%|| 29/63 [00:14<00:15,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.37416):  48%|| 30/63 [00:14<00:15,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.51336):  48%|| 30/63 [00:14<00:15,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.51336):  49%|| 31/63 [00:14<00:14,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.40786):  49%|| 31/63 [00:15<00:14,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.40786):  51%|| 32/63 [00:15<00:14,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.41093):  51%|| 32/63 [00:15<00:14,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.41093):  52%|| 33/63 [00:15<00:13,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.41029):  52%|| 33/63 [00:16<00:13,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.41029):  54%|| 34/63 [00:16<00:13,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.42887):  54%|| 34/63 [00:16<00:13,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.42887):  56%|| 35/63 [00:16<00:12,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.41958):  56%|| 35/63 [00:17<00:12,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.41958):  57%|| 36/63 [00:17<00:12,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.45589):  57%|| 36/63 [00:17<00:12,  2.17it/s]\u001b[A\n",
            "Validating... (loss=0.45589):  59%|| 37/63 [00:17<00:11,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.43947):  59%|| 37/63 [00:18<00:11,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.43947):  60%|| 38/63 [00:18<00:11,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.47750):  60%|| 38/63 [00:18<00:11,  2.15it/s]\u001b[A\n",
            "Validating... (loss=0.47750):  62%|| 39/63 [00:18<00:11,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.89755):  62%|| 39/63 [00:19<00:11,  2.14it/s]\u001b[A\n",
            "Validating... (loss=0.89755):  63%|| 40/63 [00:19<00:10,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.91290):  63%|| 40/63 [00:19<00:10,  2.12it/s]\u001b[A\n",
            "Validating... (loss=0.91290):  65%|| 41/63 [00:19<00:10,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.92599):  65%|| 41/63 [00:19<00:10,  2.13it/s]\u001b[A\n",
            "Validating... (loss=0.92599):  67%|| 42/63 [00:19<00:09,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.17082):  67%|| 42/63 [00:20<00:09,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.17082):  68%|| 43/63 [00:20<00:09,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.93624):  68%|| 43/63 [00:20<00:09,  2.16it/s]\u001b[A\n",
            "Validating... (loss=0.93624):  70%|| 44/63 [00:20<00:08,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.08544):  70%|| 44/63 [00:21<00:08,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.08544):  71%|| 45/63 [00:21<00:08,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.02192):  71%|| 45/63 [00:21<00:08,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.02192):  73%|| 46/63 [00:21<00:07,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.09098):  73%|| 46/63 [00:22<00:07,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.09098):  75%|| 47/63 [00:22<00:07,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.13417):  75%|| 47/63 [00:22<00:07,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.13417):  76%|| 48/63 [00:22<00:06,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.23030):  76%|| 48/63 [00:23<00:06,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.23030):  78%|| 49/63 [00:23<00:06,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.19458):  78%|| 49/63 [00:23<00:06,  2.15it/s]\u001b[A\n",
            "Validating... (loss=1.19458):  79%|| 50/63 [00:23<00:05,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.03999):  79%|| 50/63 [00:24<00:05,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.03999):  81%|| 51/63 [00:24<00:05,  2.18it/s]\u001b[A\n",
            "Validating... (loss=1.03413):  81%|| 51/63 [00:24<00:05,  2.18it/s]\u001b[A\n",
            "Validating... (loss=1.03413):  83%|| 52/63 [00:24<00:05,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.03189):  83%|| 52/63 [00:25<00:05,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.03189):  84%|| 53/63 [00:25<00:04,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.00062):  84%|| 53/63 [00:25<00:04,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.00062):  86%|| 54/63 [00:25<00:04,  2.13it/s]\u001b[A\n",
            "Validating... (loss=1.25775):  86%|| 54/63 [00:26<00:04,  2.13it/s]\u001b[A\n",
            "Validating... (loss=1.25775):  87%|| 55/63 [00:26<00:03,  2.14it/s]\u001b[A\n",
            "Validating... (loss=1.00348):  87%|| 55/63 [00:26<00:03,  2.14it/s]\u001b[A\n",
            "Validating... (loss=1.00348):  89%|| 56/63 [00:26<00:03,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.09386):  89%|| 56/63 [00:26<00:03,  2.16it/s]\u001b[A\n",
            "Validating... (loss=1.09386):  90%|| 57/63 [00:26<00:02,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.06520):  90%|| 57/63 [00:27<00:02,  2.17it/s]\u001b[A\n",
            "Validating... (loss=1.06520):  92%|| 58/63 [00:27<00:02,  2.18it/s]\u001b[A\n",
            "Validating... (loss=1.14713):  92%|| 58/63 [00:27<00:02,  2.18it/s]\u001b[A\n",
            "Validating... (loss=1.14713):  94%|| 59/63 [00:27<00:01,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.77146):  94%|| 59/63 [00:28<00:01,  2.18it/s]\u001b[A\n",
            "Validating... (loss=0.77146):  95%|| 60/63 [00:28<00:01,  2.20it/s]\u001b[A\n",
            "Validating... (loss=1.03586):  95%|| 60/63 [00:28<00:01,  2.20it/s]\u001b[A\n",
            "Validating... (loss=1.03586):  97%|| 61/63 [00:28<00:00,  2.19it/s]\u001b[A\n",
            "Validating... (loss=1.03154):  97%|| 61/63 [00:29<00:00,  2.19it/s]\u001b[A\n",
            "Validating... (loss=1.03154):  98%|| 62/63 [00:29<00:00,  2.20it/s]\u001b[A\n",
            "Validating... (loss=1.27121):  98%|| 62/63 [00:29<00:00,  2.20it/s]\u001b[A\n",
            "Validating... (loss=1.27121): 100%|| 63/63 [00:29<00:00,  2.13it/s]\n",
            "Training (6628 / 16570 Steps) (loss=0.42211): 100%|| 1657/1657 [18:38<00:00,  1.48it/s]\n",
            "Training (6743 / 16570 Steps) (loss=0.56550):   7%|| 115/1657 [01:16<16:52,  1.52it/s]Exception in thread Thread-20 (_pin_memory_loop):\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 1016, in _bootstrap_inner\n",
            "    self.run()\n",
            "  File \"/usr/lib/python3.10/threading.py\", line 953, in run\n",
            "    self._target(*self._args, **self._kwargs)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 51, in _pin_memory_loop\n",
            "    do_one_step()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/pin_memory.py\", line 28, in do_one_step\n",
            "    r = in_queue.get(timeout=MP_STATUS_CHECK_INTERVAL)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/queues.py\", line 122, in get\n",
            "    return _ForkingPickler.loads(res)\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/multiprocessing/reductions.py\", line 307, in rebuild_storage_fd\n",
            "    fd = df.detach()\n",
            "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 57, in detach\n",
            "    with _resource_sharer.get_connection(self._id) as conn:\n",
            "  File \"/usr/lib/python3.10/multiprocessing/resource_sharer.py\", line 86, in get_connection\n",
            "    c = Client(address, authkey=process.current_process().authkey)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 502, in Client\n",
            "    c = SocketClient(address)\n",
            "  File \"/usr/lib/python3.10/multiprocessing/connection.py\", line 630, in SocketClient\n",
            "    s.connect(address)\n",
            "FileNotFoundError: [Errno 2] No such file or directory\n",
            "Training (6743 / 16570 Steps) (loss=0.56550):   7%|| 115/1657 [01:16<17:09,  1.50it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-2a29dda38b42>\u001b[0m in \u001b[0;36m<cell line: 54>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-12-2a29dda38b42>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Training\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-90032439936d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(args, model)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 202\u001b[0;31m                 \u001b[0mlosses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_accumulation_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    203\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp16\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    204\u001b[0m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaster_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_grad_norm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AHrpeYfc8E5x"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}