{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOSqO+xkadM6W1HnNUjELNq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/visionCodes/blob/master/Train_ViT_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown https://drive.google.com/uc?id=14u9sgibcvl5-2oThkjYWlgYPvgjp0RhV"
      ],
      "metadata": {
        "id": "9F-pNaIFdRHi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4f9e4eb7-e858-468e-cbb4-181dc1f71072"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=14u9sgibcvl5-2oThkjYWlgYPvgjp0RhV\n",
            "To: /content/imgs-train+test.zip\n",
            " 18% 246M/1.36G [00:01<00:05, 199MB/s]Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/gdown\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gdown/cli.py\", line 151, in main\n",
            "    filename = download(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/gdown/download.py\", line 275, in download\n",
            "    f.write(chunk)\n",
            "KeyboardInterrupt\n",
            " 19% 259M/1.36G [00:02<00:08, 124MB/s]\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip \"/content/imgs-train+test.zip\""
      ],
      "metadata": {
        "id": "nvujlUCYdi0L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ea7b93-84cf-4743-b4b4-961014e749b2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/imgs-train+test.zip\n",
            "replace img-train+test/test/0/01285.jpg_4.png? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "6yV2GxQWZw4L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bb4a925-1a5a-4fb3-846f-da45e8f88b41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import transforms\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "KKSy44hXZx8a"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your training and validation data\n",
        "train_data_root = \"/content/img-train+test/train\"\n",
        "val_data_root = \"/content/img-train+test/test\""
      ],
      "metadata": {
        "id": "LCC0cmnIZzTH"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import ViTImageProcessor\n",
        "\n",
        "# processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "# image_mean, image_std = processor.image_mean, processor.image_std\n",
        "# size = processor.size[\"height\"]\n",
        "\n",
        "# # Define transformations for the input images\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize(size),\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=image_mean, std=image_std)\n",
        "# ])\n"
      ],
      "metadata": {
        "id": "VcVrx7RkZzdV"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, root_dir):\n",
        "        self.root_dir = root_dir\n",
        "        self.image_folder = ImageFolder(root_dir, transform=None)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_folder)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_path, label = self.image_folder.imgs[idx]\n",
        "        image = self.load_image(image_path)\n",
        "        return image, label\n",
        "\n",
        "    def load_image(self, path):\n",
        "        image = Image.open(path).convert(\"RGB\")\n",
        "        transform = transforms.Compose([\n",
        "            transforms.Resize((224, 224)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "        image = transform(image)\n",
        "        return image"
      ],
      "metadata": {
        "id": "s5__Q9jk0kZc"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset using ImageFolder and apply transformations\n",
        "train_dataset = ImageDataset(train_data_root)\n",
        "val_dataset = ImageDataset(val_data_root)"
      ],
      "metadata": {
        "id": "Ysx46DEHZznC"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label2id and id2label dictionaries based on the class names in the dataset\n",
        "label2id = {class_name: idx for class_name, idx in train_dataset.image_folder.class_to_idx.items()}\n",
        "id2label = {idx: class_name for class_name, idx in label2id.items()}"
      ],
      "metadata": {
        "id": "x0bNIiTMZzwL"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate(batch):\n",
        "    # print(\"item1: \", batch[0])\n",
        "    # print(\"item2: \", batch[1])\n",
        "\n",
        "    images = [feature_extractor(images=item[0], return_tensors=\"pt\") for item in batch]\n",
        "    images = [item['pixel_values'] for item in images]\n",
        "    labels = [torch.tensor(item[1]) for item in batch]\n",
        "\n",
        "    images, labels = zip(*batch)\n",
        "    images = torch.stack(images, dim=0)\n",
        "    labels = torch.tensor(labels)\n",
        "\n",
        "    return images, labels"
      ],
      "metadata": {
        "id": "fxNRfH0Kxjrf"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the feature extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# Define batch size and number of workers (adjust based on your system's resources)\n",
        "batch_size = 4\n",
        "num_workers = 1\n",
        "\n",
        "# Create DataLoader for the dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, collate_fn= custom_collate)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers, collate_fn= custom_collate)"
      ],
      "metadata": {
        "id": "l9zA4YBKZ0SA"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(train_dataset.image_folder.classes)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "id": "3y3_g9PHZ0by",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe431cd3-03e8-4d28-c81f-6728c3fb1980"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", id2label=id2label, label2id=label2id, num_labels=num_classes)\n",
        "vit.classifier = nn.Linear(vit.config.hidden_size, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit.to(device)\n",
        "print(vit)"
      ],
      "metadata": {
        "id": "d5Yqjhf2Z0kl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5b70e99-886d-41c9-dc07-5b6f90a3c28c"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViTForImageClassification(\n",
            "  (vit): ViTModel(\n",
            "    (embeddings): ViTEmbeddings(\n",
            "      (patch_embeddings): ViTPatchEmbeddings(\n",
            "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      )\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): ViTEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x ViTLayer(\n",
            "          (attention): ViTAttention(\n",
            "            (attention): ViTSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): ViTSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): ViTIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): ViTOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = AdamW(vit.parameters(), lr=1e-5)\n",
        "optimizer = optim.SGD(vit.parameters(), lr=0.001, momentum=0.9)\n",
        "num_epochs = 20"
      ],
      "metadata": {
        "id": "JdqAwRBSaEGC"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for epoch in range(num_epochs):\n",
        "    vit.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        # print(f\"images.shape {images.shape}\")\n",
        "        # print(f\"labels.shape {labels.shape}\")\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vit(images)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Calculate average loss for this epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    save_dir = \"/content/Train-ViT-02/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    model_name = str(epoch+1) + \"_model.pth\"\n",
        "    save_path = os.path.join(save_dir, model_name)  # Specify the complete path to the model file\n",
        "    torch.save(vit.state_dict(), save_path)\n",
        "\n",
        "    # Validation\n",
        "    vit.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "    predicted_classes = []\n",
        "    actual_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = vit(images)\n",
        "            val_loss += criterion(outputs.logits, labels).item() * images.size(0)\n",
        "\n",
        "            probabilities = torch.softmax(outputs.logits, dim=1)\n",
        "            predicted = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            predicted_classes.extend(predicted.cpu().numpy())\n",
        "            actual_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average loss and accuracy for validation set\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Compute evaluation metrics using the predicted_classes and actual_labels lists\n",
        "    accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "    precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "    recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "    f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "    print(classification_report(actual_labels, predicted_classes))\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(confusion_matrix(actual_labels, predicted_classes))\n",
        "    print(\"AUROC:\", roc_auc_score(actual_labels, predicted_classes))\n",
        "\n"
      ],
      "metadata": {
        "id": "NVhg3NjSaGwr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8459b672-8dce-4db9-ecc4-260f8e750c9f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Training Loss: 0.6359 - Validation Loss: 0.6308 - Accuracy: 0.6485\n",
            "Accuracy: 0.6485\n",
            "Precision: 0.6560787090909803\n",
            "Recall: 0.6485\n",
            "F1-score: 0.5616483191735859\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.96      0.77      1250\n",
            "           1       0.67      0.12      0.21       750\n",
            "\n",
            "    accuracy                           0.65      2000\n",
            "   macro avg       0.66      0.54      0.49      2000\n",
            "weighted avg       0.66      0.65      0.56      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1205   45]\n",
            " [ 658   92]]\n",
            "AUROC: 0.5433333333333333\n",
            "Epoch 2/20 - Training Loss: 0.6012 - Validation Loss: 0.6660 - Accuracy: 0.6045\n",
            "Accuracy: 0.6265\n",
            "Precision: 0.589772159554048\n",
            "Recall: 0.6265\n",
            "F1-score: 0.5491536312236911\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.93      0.76      2500\n",
            "           1       0.51      0.13      0.20      1500\n",
            "\n",
            "    accuracy                           0.63      4000\n",
            "   macro avg       0.57      0.53      0.48      4000\n",
            "weighted avg       0.59      0.63      0.55      4000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2314  186]\n",
            " [1308  192]]\n",
            "AUROC: 0.5268\n",
            "Epoch 3/20 - Training Loss: 0.6368 - Validation Loss: 0.6442 - Accuracy: 0.6405\n",
            "Accuracy: 0.6311666666666667\n",
            "Precision: 0.6004500249712288\n",
            "Recall: 0.6311666666666667\n",
            "F1-score: 0.5614336767647452\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.92      0.76      3750\n",
            "           1       0.53      0.15      0.24      2250\n",
            "\n",
            "    accuracy                           0.63      6000\n",
            "   macro avg       0.59      0.54      0.50      6000\n",
            "weighted avg       0.60      0.63      0.56      6000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3446  304]\n",
            " [1909  341]]\n",
            "AUROC: 0.5352444444444444\n",
            "Epoch 4/20 - Training Loss: 0.5880 - Validation Loss: 0.6315 - Accuracy: 0.6480\n",
            "Accuracy: 0.635375\n",
            "Precision: 0.6090182486519056\n",
            "Recall: 0.635375\n",
            "F1-score: 0.5684305258588698\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.92      0.76      5000\n",
            "           1       0.55      0.16      0.25      3000\n",
            "\n",
            "    accuracy                           0.64      8000\n",
            "   macro avg       0.60      0.54      0.50      8000\n",
            "weighted avg       0.61      0.64      0.57      8000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4595  405]\n",
            " [2512  488]]\n",
            "AUROC: 0.5408333333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4dPB8YABeK7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}