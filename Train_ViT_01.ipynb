{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMcpjsIfFLtHysZtXnAJy6+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/visionCodes/blob/master/Train_ViT_01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# google/vit-base-patch16-224-in21k on fused train dataset using SGD 0.001"
      ],
      "metadata": {
        "id": "quQ4Wf-5rN4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !gdown https://drive.google.com/uc?id=1wgl3QGXZ4m2aLg3T-1TDXQqSP31RuXgL"
      ],
      "metadata": {
        "id": "9F-pNaIFdRHi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !unzip \"/content/hateful_train+test_unseen.zip\""
      ],
      "metadata": {
        "id": "nvujlUCYdi0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6yV2GxQWZw4L",
        "outputId": "4ef36129-80db-4049-c1f4-7f8931f15511"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision.transforms import transforms\n",
        "from transformers import ViTForImageClassification, ViTFeatureExtractor, AdamW\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix, roc_auc_score\n",
        "from PIL import Image"
      ],
      "metadata": {
        "id": "KKSy44hXZx8a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your training and validation data\n",
        "train_data_root = \"/content/hateful_ViT1/train\"\n",
        "val_data_root = \"/content/hateful_ViT1/test\""
      ],
      "metadata": {
        "id": "LCC0cmnIZzTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import ViTImageProcessor\n",
        "\n",
        "processor = ViTImageProcessor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "image_mean, image_std = processor.image_mean, processor.image_std\n",
        "size = processor.size[\"height\"]\n",
        "\n",
        "# Define transformations for the input images\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize(size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=image_mean, std=image_std)\n",
        "])\n"
      ],
      "metadata": {
        "id": "VcVrx7RkZzdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the dataset using ImageFolder and apply transformations\n",
        "train_dataset = ImageFolder(train_data_root, transform=transform)\n",
        "val_dataset = ImageFolder(val_data_root, transform=transform)"
      ],
      "metadata": {
        "id": "Ysx46DEHZznC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create label2id and id2label dictionaries based on the class names in the dataset\n",
        "label2id = {class_name: idx for class_name, idx in train_dataset.class_to_idx.items()}\n",
        "id2label = {idx: class_name for class_name, idx in train_dataset.class_to_idx.items()}"
      ],
      "metadata": {
        "id": "x0bNIiTMZzwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the feature extractor\n",
        "feature_extractor = ViTFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
        "\n",
        "# Define batch size and number of workers (adjust based on your system's resources)\n",
        "batch_size = 32\n",
        "num_workers = 1\n",
        "\n",
        "# Create DataLoader for the dataset\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l9zA4YBKZ0SA",
        "outputId": "ee2d1e10-93cd-4a10-8850-03599f08b804"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_classes = len(train_dataset.classes)\n",
        "print(num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3y3_g9PHZ0by",
        "outputId": "9d90e662-e9fb-4efe-a036-e1ac21c4237d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit = ViTForImageClassification.from_pretrained(\"google/vit-base-patch16-224-in21k\", id2label=id2label, label2id=label2id)\n",
        "vit.classifier = nn.Linear(vit.config.hidden_size, num_classes)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "vit.to(device)\n",
        "print(vit)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5Yqjhf2Z0kl",
        "outputId": "ca7f5afc-d37e-411c-d20c-989c833db447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViTForImageClassification(\n",
            "  (vit): ViTModel(\n",
            "    (embeddings): ViTEmbeddings(\n",
            "      (patch_embeddings): ViTPatchEmbeddings(\n",
            "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
            "      )\n",
            "      (dropout): Dropout(p=0.0, inplace=False)\n",
            "    )\n",
            "    (encoder): ViTEncoder(\n",
            "      (layer): ModuleList(\n",
            "        (0-11): 12 x ViTLayer(\n",
            "          (attention): ViTAttention(\n",
            "            (attention): ViTSelfAttention(\n",
            "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (output): ViTSelfOutput(\n",
            "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (intermediate): ViTIntermediate(\n",
            "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "            (intermediate_act_fn): GELUActivation()\n",
            "          )\n",
            "          (output): ViTOutput(\n",
            "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.0, inplace=False)\n",
            "          )\n",
            "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer = AdamW(vit.parameters(), lr=1e-5)\n",
        "optimizer = optim.SGD(vit.parameters(), lr=0.001, momentum=0.9)\n",
        "num_epochs = 20"
      ],
      "metadata": {
        "id": "JdqAwRBSaEGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes = []\n",
        "actual_labels = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    vit.train()\n",
        "    train_loss = 0.0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = vit(images).logits\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item() * images.size(0)\n",
        "\n",
        "    # Calculate average loss for this epoch\n",
        "    train_loss /= len(train_loader.dataset)\n",
        "\n",
        "    save_dir = \"/content/Train-ViT-01/\"\n",
        "    os.makedirs(save_dir, exist_ok=True)  # Create the directory if it doesn't exist\n",
        "\n",
        "    model_name = str(epoch+1) + \"_model.pth\"\n",
        "    save_path = os.path.join(save_dir, model_name)  # Specify the complete path to the model file\n",
        "    torch.save(vit.state_dict(), save_path)\n",
        "\n",
        "    # Validation\n",
        "    vit.eval()\n",
        "    val_loss = 0.0\n",
        "    correct = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in val_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = vit(images).logits\n",
        "            val_loss += criterion(outputs, labels).item() * images.size(0)\n",
        "\n",
        "            probabilities = torch.softmax(outputs, dim=1)\n",
        "            predicted = torch.argmax(probabilities, dim=1)\n",
        "\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            predicted_classes.extend(predicted.cpu().numpy())\n",
        "            actual_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average loss and accuracy for validation set\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{num_epochs} - Training Loss: {train_loss:.4f} - Validation Loss: {val_loss:.4f} - Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "    # Compute evaluation metrics using the predicted_classes and actual_labels lists\n",
        "    accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "    precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "    recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "    f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "    print(\"Accuracy:\", accuracy)\n",
        "    print(\"Precision:\", precision)\n",
        "    print(\"Recall:\", recall)\n",
        "    print(\"F1-score:\", f1)\n",
        "    print(classification_report(actual_labels, predicted_classes))\n",
        "    cm = confusion_matrix(actual_labels, predicted_classes)\n",
        "    print(\"Confusion Matrix:\")\n",
        "    print(cm)\n",
        "    auroc = roc_auc_score(actual_labels, predicted_classes)\n",
        "    print(\"AUROC:\", auroc)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NVhg3NjSaGwr",
        "outputId": "57f43fc2-6dcc-47b1-a05f-25239bfcc30c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20 - Training Loss: 0.6384 - Validation Loss: 0.6469 - Accuracy: 0.6355\n",
            "Accuracy: 0.6355\n",
            "Precision: 0.6191898055498133\n",
            "Recall: 0.6355\n",
            "F1-score: 0.5405230099961142\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.64      0.96      0.77      1250\n",
            "           1       0.59      0.09      0.16       750\n",
            "\n",
            "    accuracy                           0.64      2000\n",
            "   macro avg       0.61      0.53      0.47      2000\n",
            "weighted avg       0.62      0.64      0.54      2000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[1200   50]\n",
            " [ 679   71]]\n",
            "AUROC: 0.5273333333333333\n",
            "Epoch 2/20 - Training Loss: 0.5923 - Validation Loss: 0.6424 - Accuracy: 0.6460\n",
            "Accuracy: 0.64075\n",
            "Precision: 0.6252748433253531\n",
            "Recall: 0.64075\n",
            "F1-score: 0.5617393213623161\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.94      0.77      2500\n",
            "           1       0.59      0.14      0.22      1500\n",
            "\n",
            "    accuracy                           0.64      4000\n",
            "   macro avg       0.62      0.54      0.49      4000\n",
            "weighted avg       0.63      0.64      0.56      4000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2360  140]\n",
            " [1297  203]]\n",
            "AUROC: 0.5396666666666666\n",
            "Epoch 3/20 - Training Loss: 0.5459 - Validation Loss: 0.6340 - Accuracy: 0.6585\n",
            "Accuracy: 0.6466666666666666\n",
            "Precision: 0.6312535822484238\n",
            "Recall: 0.6466666666666666\n",
            "F1-score: 0.5833445961165924\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.65      0.93      0.77      3750\n",
            "           1       0.59      0.18      0.28      2250\n",
            "\n",
            "    accuracy                           0.65      6000\n",
            "   macro avg       0.62      0.55      0.52      6000\n",
            "weighted avg       0.63      0.65      0.58      6000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[3470  280]\n",
            " [1840  410]]\n",
            "AUROC: 0.5537777777777778\n",
            "Epoch 4/20 - Training Loss: 0.4927 - Validation Loss: 0.6444 - Accuracy: 0.6525\n",
            "Accuracy: 0.648125\n",
            "Precision: 0.6311287509086094\n",
            "Recall: 0.648125\n",
            "F1-score: 0.5925907855777479\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.66      0.91      0.76      5000\n",
            "           1       0.59      0.21      0.31      3000\n",
            "\n",
            "    accuracy                           0.65      8000\n",
            "   macro avg       0.62      0.56      0.54      8000\n",
            "weighted avg       0.63      0.65      0.59      8000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[4563  437]\n",
            " [2378  622]]\n",
            "AUROC: 0.5599666666666667\n",
            "Epoch 5/20 - Training Loss: 0.4207 - Validation Loss: 0.6588 - Accuracy: 0.6635\n",
            "Accuracy: 0.6512\n",
            "Precision: 0.6329152536494437\n",
            "Recall: 0.6512\n",
            "F1-score: 0.6099690692653411\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.89      0.76      6250\n",
            "           1       0.58      0.26      0.36      3750\n",
            "\n",
            "    accuracy                           0.65     10000\n",
            "   macro avg       0.62      0.57      0.56     10000\n",
            "weighted avg       0.63      0.65      0.61     10000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[5534  716]\n",
            " [2772  978]]\n",
            "AUROC: 0.57312\n",
            "Epoch 6/20 - Training Loss: 0.3363 - Validation Loss: 0.6685 - Accuracy: 0.6940\n",
            "Accuracy: 0.6583333333333333\n",
            "Precision: 0.6431256553241869\n",
            "Recall: 0.6583333333333333\n",
            "F1-score: 0.6193985411139888\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.89      0.76      7500\n",
            "           1       0.60      0.28      0.38      4500\n",
            "\n",
            "    accuracy                           0.66     12000\n",
            "   macro avg       0.63      0.58      0.57     12000\n",
            "weighted avg       0.64      0.66      0.62     12000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[6657  843]\n",
            " [3257 1243]]\n",
            "AUROC: 0.581911111111111\n",
            "Epoch 7/20 - Training Loss: 0.2539 - Validation Loss: 0.7228 - Accuracy: 0.6760\n",
            "Accuracy: 0.6608571428571428\n",
            "Precision: 0.6451899929195185\n",
            "Recall: 0.6608571428571428\n",
            "F1-score: 0.63106458217011\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.68      0.87      0.76      8750\n",
            "           1       0.59      0.32      0.41      5250\n",
            "\n",
            "    accuracy                           0.66     14000\n",
            "   macro avg       0.63      0.59      0.59     14000\n",
            "weighted avg       0.65      0.66      0.63     14000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[7576 1174]\n",
            " [3574 1676]]\n",
            "AUROC: 0.5925333333333332\n",
            "Epoch 8/20 - Training Loss: 0.1928 - Validation Loss: 0.7541 - Accuracy: 0.6940\n",
            "Accuracy: 0.665\n",
            "Precision: 0.6502681864797283\n",
            "Recall: 0.665\n",
            "F1-score: 0.6410108538499385\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.85      0.76     10000\n",
            "           1       0.59      0.35      0.44      6000\n",
            "\n",
            "    accuracy                           0.67     16000\n",
            "   macro avg       0.64      0.60      0.60     16000\n",
            "weighted avg       0.65      0.67      0.64     16000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[8522 1478]\n",
            " [3882 2118]]\n",
            "AUROC: 0.6026\n",
            "Epoch 9/20 - Training Loss: 0.1548 - Validation Loss: 0.8408 - Accuracy: 0.7140\n",
            "Accuracy: 0.6704444444444444\n",
            "Precision: 0.6569537589061419\n",
            "Recall: 0.6704444444444444\n",
            "F1-score: 0.649547372379729\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.69      0.85      0.76     11250\n",
            "           1       0.60      0.38      0.46      6750\n",
            "\n",
            "    accuracy                           0.67     18000\n",
            "   macro avg       0.64      0.61      0.61     18000\n",
            "weighted avg       0.66      0.67      0.65     18000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[9531 1719]\n",
            " [4213 2537]]\n",
            "AUROC: 0.6115259259259259\n",
            "Epoch 10/20 - Training Loss: 0.1159 - Validation Loss: 0.8702 - Accuracy: 0.7255\n",
            "Accuracy: 0.67595\n",
            "Precision: 0.6635580382020324\n",
            "Recall: 0.67595\n",
            "F1-score: 0.6572507281328399\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.84      0.77     12500\n",
            "           1       0.60      0.39      0.48      7500\n",
            "\n",
            "    accuracy                           0.68     20000\n",
            "   macro avg       0.65      0.62      0.62     20000\n",
            "weighted avg       0.66      0.68      0.66     20000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[10559  1941]\n",
            " [ 4540  2960]]\n",
            "AUROC: 0.6196933333333333\n",
            "Epoch 11/20 - Training Loss: 0.0882 - Validation Loss: 0.8693 - Accuracy: 0.7330\n",
            "Accuracy: 0.6811363636363637\n",
            "Precision: 0.6696724511092618\n",
            "Recall: 0.6811363636363637\n",
            "F1-score: 0.6643497901084726\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.84      0.77     13750\n",
            "           1       0.61      0.41      0.49      8250\n",
            "\n",
            "    accuracy                           0.68     22000\n",
            "   macro avg       0.66      0.63      0.63     22000\n",
            "weighted avg       0.67      0.68      0.66     22000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[11584  2166]\n",
            " [ 4849  3401]]\n",
            "AUROC: 0.6273575757575758\n",
            "Epoch 12/20 - Training Loss: 0.0666 - Validation Loss: 0.9437 - Accuracy: 0.7465\n",
            "Accuracy: 0.6865833333333333\n",
            "Precision: 0.6759809250273379\n",
            "Recall: 0.6865833333333333\n",
            "F1-score: 0.6709993454863455\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.84      0.77     15000\n",
            "           1       0.62      0.43      0.50      9000\n",
            "\n",
            "    accuracy                           0.69     24000\n",
            "   macro avg       0.66      0.63      0.64     24000\n",
            "weighted avg       0.68      0.69      0.67     24000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[12645  2355]\n",
            " [ 5167  3833]]\n",
            "AUROC: 0.6344444444444444\n",
            "Epoch 13/20 - Training Loss: 0.0655 - Validation Loss: 0.9985 - Accuracy: 0.7200\n",
            "Accuracy: 0.6891538461538461\n",
            "Precision: 0.6790165950333822\n",
            "Recall: 0.6891538461538461\n",
            "F1-score: 0.6757213784676269\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.84      0.77     16250\n",
            "           1       0.62      0.44      0.52      9750\n",
            "\n",
            "    accuracy                           0.69     26000\n",
            "   macro avg       0.67      0.64      0.64     26000\n",
            "weighted avg       0.68      0.69      0.68     26000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[13587  2663]\n",
            " [ 5419  4331]]\n",
            "AUROC: 0.6401641025641026\n",
            "Epoch 14/20 - Training Loss: 0.0543 - Validation Loss: 1.1489 - Accuracy: 0.6900\n",
            "Accuracy: 0.6892142857142857\n",
            "Precision: 0.6794902461817743\n",
            "Recall: 0.6892142857142857\n",
            "F1-score: 0.6781289626800732\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.83      0.77     17500\n",
            "           1       0.61      0.46      0.53     10500\n",
            "\n",
            "    accuracy                           0.69     28000\n",
            "   macro avg       0.67      0.64      0.65     28000\n",
            "weighted avg       0.68      0.69      0.68     28000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[14438  3062]\n",
            " [ 5640  4860]]\n",
            "AUROC: 0.6439428571428572\n",
            "Epoch 15/20 - Training Loss: 0.0344 - Validation Loss: 1.0369 - Accuracy: 0.7375\n",
            "Accuracy: 0.6924333333333333\n",
            "Precision: 0.6832793774938666\n",
            "Recall: 0.6924333333333333\n",
            "F1-score: 0.6824507232322791\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.82      0.77     18750\n",
            "           1       0.62      0.48      0.54     11250\n",
            "\n",
            "    accuracy                           0.69     30000\n",
            "   macro avg       0.67      0.65      0.65     30000\n",
            "weighted avg       0.68      0.69      0.68     30000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[15423  3327]\n",
            " [ 5900  5350]]\n",
            "AUROC: 0.6490577777777777\n",
            "Epoch 16/20 - Training Loss: 0.0444 - Validation Loss: 1.1381 - Accuracy: 0.7540\n",
            "Accuracy: 0.69628125\n",
            "Precision: 0.6875436567671132\n",
            "Recall: 0.69628125\n",
            "F1-score: 0.686713617548099\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.82      0.77     20000\n",
            "           1       0.62      0.48      0.54     12000\n",
            "\n",
            "    accuracy                           0.70     32000\n",
            "   macro avg       0.67      0.65      0.66     32000\n",
            "weighted avg       0.69      0.70      0.69     32000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[16482  3518]\n",
            " [ 6201  5799]]\n",
            "AUROC: 0.653675\n",
            "Epoch 17/20 - Training Loss: 0.3195 - Validation Loss: 0.6580 - Accuracy: 0.6230\n",
            "Accuracy: 0.6919705882352941\n",
            "Precision: 0.6824987836215043\n",
            "Recall: 0.6919705882352941\n",
            "F1-score: 0.6808960592120442\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.83      0.77     21250\n",
            "           1       0.62      0.47      0.53     12750\n",
            "\n",
            "    accuracy                           0.69     34000\n",
            "   macro avg       0.67      0.65      0.65     34000\n",
            "weighted avg       0.68      0.69      0.68     34000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[17588  3662]\n",
            " [ 6811  5939]]\n",
            "AUROC: 0.6467372549019609\n",
            "Epoch 18/20 - Training Loss: 0.5791 - Validation Loss: 0.6549 - Accuracy: 0.6455\n",
            "Accuracy: 0.6893888888888889\n",
            "Precision: 0.6793749516819166\n",
            "Recall: 0.6893888888888889\n",
            "F1-score: 0.6767449517415931\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.72      0.83      0.77     22500\n",
            "           1       0.62      0.45      0.52     13500\n",
            "\n",
            "    accuracy                           0.69     36000\n",
            "   macro avg       0.67      0.64      0.65     36000\n",
            "weighted avg       0.68      0.69      0.68     36000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[18736  3764]\n",
            " [ 7418  6082]]\n",
            "AUROC: 0.6416148148148149\n",
            "Epoch 19/20 - Training Loss: 0.5157 - Validation Loss: 0.6763 - Accuracy: 0.6485\n",
            "Accuracy: 0.6872368421052631\n",
            "Precision: 0.676967868251042\n",
            "Recall: 0.6872368421052631\n",
            "F1-score: 0.6744690973563982\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.83      0.77     23750\n",
            "           1       0.61      0.45      0.52     14250\n",
            "\n",
            "    accuracy                           0.69     38000\n",
            "   macro avg       0.66      0.64      0.64     38000\n",
            "weighted avg       0.68      0.69      0.67     38000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[19740  4010]\n",
            " [ 7875  6375]]\n",
            "AUROC: 0.6392631578947369\n",
            "Epoch 20/20 - Training Loss: 0.4405 - Validation Loss: 0.6670 - Accuracy: 0.6620\n",
            "Accuracy: 0.685975\n",
            "Precision: 0.6755018928017397\n",
            "Recall: 0.685975\n",
            "F1-score: 0.6728143521858501\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.71      0.83      0.77     25000\n",
            "           1       0.61      0.44      0.51     15000\n",
            "\n",
            "    accuracy                           0.69     40000\n",
            "   macro avg       0.66      0.64      0.64     40000\n",
            "weighted avg       0.68      0.69      0.67     40000\n",
            "\n",
            "Confusion Matrix:\n",
            "[[20793  4207]\n",
            " [ 8354  6646]]\n",
            "AUROC: 0.6373933333333334\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4dPB8YABeK7K"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}