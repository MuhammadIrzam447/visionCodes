{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "qKJ68HYxf5iq",
        "4GQRp3F5Hbob",
        "00vlqtj_0TXq",
        "vsW7ms9iFQIi",
        "GZDNW0tS225-",
        "tH3B0YYuXbh3",
        "oA19HxxWkW78"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MuhammadIrzam447/visionCodes/blob/master/Debugging_Base.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Testing Multi-model performance for 100% images and 100% text.\n",
        "# Updated Validation Dataset Class, DataLoader and Validation Loop\n",
        "# Uncomment the comments in Validation Dataset, and Custom Collate_fun() to observe data loading process."
      ],
      "metadata": {
        "id": "VhXCwAGe33A1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ObgVtPmABT7d"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image, UnidentifiedImageError\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.models as models\n",
        "from torchvision.datasets import ImageFolder\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support, accuracy_score, precision_score, recall_score, f1_score\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "id": "pziXvdPgU58B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading Validation Dataset and Preprocessing"
      ],
      "metadata": {
        "id": "p3yzu25SYhKf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = ''\n",
        "load_path = os.path.join(save_dir, 'model.pth')\n",
        "\n",
        "# Create an instance of the ResNet model\n",
        "resnet = torchvision.models.resnet101(pretrained=False)\n",
        "resnet.fc = nn.Linear(2048, 52) # Choose the number of output classses as per your model\n",
        "\n",
        "# Load the saved model parameters\n",
        "resnet.load_state_dict(torch.load(load_path))\n",
        "\n",
        "# Set the model to evaluation mode and respective device\n",
        "resnet.eval()\n",
        "resnet.to(device)"
      ],
      "metadata": {
        "id": "6gITvuS1n91U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "])"
      ],
      "metadata": {
        "id": "4ElOxuNZvqLy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class ValidationDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, data_dir, val_transform):\n",
        "        self.data_dir = data_dir\n",
        "        self.dataset = datasets.ImageFolder(data_dir)\n",
        "        self.classes = self.dataset.classes\n",
        "        self.val_transform = val_transform\n",
        "\n",
        "        self.selected_indices = []\n",
        "        for class_idx in range(len(self.classes)):\n",
        "            indices = [idx for idx, (_, label) in enumerate(self.dataset.samples) if label == class_idx]\n",
        "            indices_3 = [idx for idx in indices if self.dataset.samples[idx][0].endswith(\"_3.png\")]    # _3.png are the encoded_text images\n",
        "            indices_4 = [idx for idx in indices if self.dataset.samples[idx][0].endswith(\"_4.png\")]    # _4.png are the actual images\n",
        "\n",
        "            self.selected_indices.extend(indices_4)\n",
        "            # self.selected_indices.extend(indices_3)\n",
        "        print(\"Selected Indices:\", len(self.selected_indices))\n",
        "        # for idx in self.selected_indices:\n",
        "          # print(self.dataset.samples[idx][0])\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        # print(\"Entered get_item\")\n",
        "        img, label = self.dataset[self.selected_indices[index]]\n",
        "        filename = self.dataset.samples[self.selected_indices[index]][0]\n",
        "        image_3 = None\n",
        "        image_4 = None\n",
        "        if filename.endswith(\"_4.png\"):\n",
        "                image_4 = img\n",
        "                image_4_path = filename\n",
        "                # print(\"image_4_path: \", str(filename))\n",
        "                image_3_path = filename.replace('_4.png', '_3.png')\n",
        "                # print(\"image_3_path: \", str(image_3_path))\n",
        "                image_3 = self._load_image(image_3_path)\n",
        "        else:\n",
        "                image_3 = img\n",
        "                image_3_path = filename\n",
        "                # print(\"image_3_path: \", str(filename))\n",
        "                image_4_path = filename.replace('_3.png', '_4.png')\n",
        "                # print(\"image_4_path: \", str(image_4_path))\n",
        "                image_4 = self._load_image(image_4_path)\n",
        "\n",
        "        # print(\"Exit get_item\")\n",
        "        return image_3, image_4, label, image_3_path, image_4_path\n",
        "\n",
        "    def _load_image(self, path):\n",
        "        image = Image.open(path)\n",
        "        return image\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.selected_indices)"
      ],
      "metadata": {
        "id": "ZEqVuNvcMgvo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "valPath = \"\"\n",
        "val_dataset = ValidationDataset(valPath,val_transform)"
      ],
      "metadata": {
        "id": "4UfrxOQ3sCgB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_collate(batch):\n",
        "    to_tensor = transforms.ToTensor()\n",
        "    val_transform = transforms.Compose([\n",
        "        transforms.Resize((224, 224)),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "    images_3 = [val_transform(item[0]) for item in batch]\n",
        "    images_4 = [val_transform(item[1]) for item in batch]\n",
        "    labels = [torch.tensor(item[2]) for item in batch]\n",
        "\n",
        "    images_3 = torch.stack(images_3)\n",
        "    images_4 = torch.stack(images_4)\n",
        "\n",
        "    # Print the filenames in each list\n",
        "    # print(\"Filenames in images_3 list:\")\n",
        "    # for item in batch:\n",
        "    #     if item[0] is not None:\n",
        "    #         print(item[3])\n",
        "\n",
        "    # print(\"Filenames in images_4 list:\")\n",
        "    # for item in batch:\n",
        "    #     if item[1] is not None:\n",
        "    #         print(item[4])\n",
        "\n",
        "    return images_3, images_4, labels\n"
      ],
      "metadata": {
        "id": "snzYnvozBPgC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 12\n",
        "validation_data_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=custom_collate)"
      ],
      "metadata": {
        "id": "8e-uQ6lqVhmI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Number of samples:\", len(val_dataset))\n",
        "print(\"Number of classes:\", len(val_dataset.classes))"
      ],
      "metadata": {
        "id": "oShp-fOcVmYs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_batches = len(validation_data_loader)\n",
        "print(\"Number of batches:\", num_batches)"
      ],
      "metadata": {
        "id": "l_2_YsDRCJL7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Validation"
      ],
      "metadata": {
        "id": "RbwN0R_b2tMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "resnet.eval()\n",
        "\n",
        "# Initialize lists to store the average probabilities and true labels\n",
        "predicted_labels = []\n",
        "true_labels = []\n",
        "\n",
        "# Iterate over the dataloader in your testing loop\n",
        "for images_3, images_4, labels in validation_data_loader:\n",
        "\n",
        "    images_3 = images_3.to(device)\n",
        "    images_4 = images_4.to(device)\n",
        "\n",
        "    logits_3 = resnet(images_3)\n",
        "    logits_4 = resnet(images_4)\n",
        "    # logits_4 = torch.zeros_like(logits_3)\n",
        "\n",
        "    probabilities_3 = torch.softmax(logits_3, dim=1)\n",
        "    probabilities_4 = torch.softmax(logits_4, dim=1)\n",
        "\n",
        "    avg_probabilities_batch = (probabilities_3 + probabilities_4) / 2\n",
        "\n",
        "    predicted_labels.extend(avg_probabilities_batch.cpu().tolist())\n",
        "\n",
        "    true_labels.extend(labels)"
      ],
      "metadata": {
        "id": "QYxUnUjO7DOa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_classes = torch.argmax(torch.tensor(predicted_labels), dim=1)\n",
        "actual_labels = torch.tensor(true_labels)"
      ],
      "metadata": {
        "id": "9PWM8Xhck2Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(actual_labels, predicted_classes)\n",
        "precision = precision_score(actual_labels, predicted_classes, average='weighted')\n",
        "recall = recall_score(actual_labels, predicted_classes, average='weighted')\n",
        "f1 = f1_score(actual_labels, predicted_classes, average='weighted')\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)"
      ],
      "metadata": {
        "id": "SiJYzy3heKgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(actual_labels, predicted_classes))"
      ],
      "metadata": {
        "id": "1PEdbnvj0vPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = ['Accuracy', 'Precision', 'Recall', 'F1-score']\n",
        "values = [accuracy, precision, recall, f1]\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(5, 4))\n",
        "bars = ax.bar(x, values)\n",
        "\n",
        "# labels, title, and legend\n",
        "ax.set_xlabel('Metrics')\n",
        "ax.set_ylabel('Score')\n",
        "ax.set_title('Model Performance Metrics')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "# scores on top of each bar\n",
        "for i, bar in enumerate(bars):\n",
        "    score = values[i]\n",
        "    ax.text(bar.get_x() + bar.get_width() / 2, bar.get_height(), f'{score:.2f}', ha='center', va='bottom')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xSO-PViWdHwB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cm = confusion_matrix(actual_labels, predicted_classes)\n",
        "ConfusionMatrixDisplay(cm).plot()"
      ],
      "metadata": {
        "id": "SE9rkrgFn0hL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count the occurrences of each label in predicted and true labels\n",
        "predicted_counts = np.bincount(predicted_classes)\n",
        "true_counts = np.bincount(actual_labels)\n",
        "\n",
        "# Get the unique labels\n",
        "labels = np.unique(np.concatenate((predicted_classes, actual_labels)))\n",
        "\n",
        "# Set the x-axis range\n",
        "x = np.arange(len(labels))\n",
        "\n",
        "# Set the width of the bars\n",
        "width = 0.35\n",
        "\n",
        "# Plot the predicted and true label counts\n",
        "fig, ax = plt.subplots(figsize=(20, 8))\n",
        "ax.bar(x - width/2, predicted_counts, width, label='Predicted Labels')\n",
        "ax.bar(x + width/2, true_counts, width, label='True Labels')\n",
        "\n",
        "# Add labels, title, and legend\n",
        "ax.set_xlabel('Labels')\n",
        "ax.set_ylabel('Count')\n",
        "ax.set_title('Distribution of Predicted and True Labels')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "ax.legend()\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "55bbdFyXajU8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}